{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention Robustness: Final Models Demo\n",
    "\n",
    "This notebook demonstrates how to load and use the final trained models from the paper:\n",
    "- **\"Inherent Robustness of Multi-Head Attention in Cross-Sectional Asset Pricing: Theory and Empirical Evidence from Finance-Valid Adversarial Attacks\"**\n",
    "\n",
    "## Reproducibility\n",
    "\n",
    "**Random seed is set to 42** for reproducible results. All models are evaluated in deterministic mode (dropout disabled). Results should be identical across runs.\n",
    "\n",
    "## Data Splitting\n",
    "\n",
    "This notebook uses the **same data splitting logic** as the evaluation script (`scripts/evaluate_adversarial_models.py`):\n",
    "- **Training period**: 2005-01-01 to 2017-12-31\n",
    "- **Validation period**: 2018-01-01 to 2019-12-31\n",
    "- **Data preprocessing**: Matches the `CrossSectionalDataSplitter` class from the evaluation script\n",
    "\n",
    "## Models Available\n",
    "\n",
    "1. **Linear Baselines**: OLS, Ridge\n",
    "2. **Neural Baselines**: MLP\n",
    "3. **Transformer Models**: Single-Head, Multi-Head, Multi-Head Diversity\n",
    "4. **Adversarially Trained Models**: Models trained with A1, A2, A3 attacks at various epsilons\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Load data\n",
    "2. **Train models from scratch** (OLS, Ridge, MLP, Single-Head, Multi-Head, Multi-Head Diversity)\n",
    "3. **Adversarial training** for transformer models (A1-A4 attacks)\n",
    "4. Compare standard vs adversarially trained models\n",
    "5. Make predictions on validation set\n",
    "6. Evaluate model performance (RMSE, R²)\n",
    "7. Visualize predictions and training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set random seed to 42 for reproducibility\n",
      "Working directory: /Users/zelalemabahana/multihead-attention-robustness/notebooks\n",
      "Repository root: /Users/zelalemabahana/multihead-attention-robustness\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Set random seed to {RANDOM_SEED} for reproducibility\")\n",
    "\n",
    "# Add parent directory to path\n",
    "repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Import model definitions\n",
    "from src.models.feature_token_transformer import FeatureTokenTransformer, SingleHeadTransformer\n",
    "\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(f\"Repository root: {repo_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/zelalemabahana/multihead-attention-robustness/data/cross_sectional/master_table.csv\n",
      "Data shape: (31534, 27)\n",
      "Columns: ['date', 'symbol', 'mom_1m', 'mom_6m', 'mom_12m', 'mom_12_1m', 'vol_3m', 'vol_12m', 'price', 'log_price']...\n",
      "\n",
      "Date range: 2005-01-31 00:00:00 to 2025-12-31 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>mom_1m</th>\n",
       "      <th>mom_6m</th>\n",
       "      <th>mom_12m</th>\n",
       "      <th>mom_12_1m</th>\n",
       "      <th>vol_3m</th>\n",
       "      <th>vol_12m</th>\n",
       "      <th>price</th>\n",
       "      <th>log_price</th>\n",
       "      <th>returns_1d</th>\n",
       "      <th>...</th>\n",
       "      <th>pb_ratio</th>\n",
       "      <th>dividend_yield</th>\n",
       "      <th>eps</th>\n",
       "      <th>roe</th>\n",
       "      <th>profit_margin</th>\n",
       "      <th>revenue_per_share</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>covid_period</th>\n",
       "      <th>ret_fwd_1m</th>\n",
       "      <th>mktcap</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-31</th>\n",
       "      <td>AAP</td>\n",
       "      <td>-0.007141</td>\n",
       "      <td>0.161412</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.114541</td>\n",
       "      <td>0.177014</td>\n",
       "      <td>0.272406</td>\n",
       "      <td>24.240944</td>\n",
       "      <td>3.188043</td>\n",
       "      <td>0.012926</td>\n",
       "      <td>...</td>\n",
       "      <td>1.126753</td>\n",
       "      <td>2.42</td>\n",
       "      <td>-10.19</td>\n",
       "      <td>-0.23868</td>\n",
       "      <td>-0.04369</td>\n",
       "      <td>144.046</td>\n",
       "      <td>2.474117e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168909</td>\n",
       "      <td>2.474117e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-31</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>-0.054833</td>\n",
       "      <td>0.176429</td>\n",
       "      <td>0.130269</td>\n",
       "      <td>0.185102</td>\n",
       "      <td>0.303383</td>\n",
       "      <td>0.286354</td>\n",
       "      <td>12.617585</td>\n",
       "      <td>2.535091</td>\n",
       "      <td>0.020060</td>\n",
       "      <td>...</td>\n",
       "      <td>1.307446</td>\n",
       "      <td>1.04</td>\n",
       "      <td>5.77</td>\n",
       "      <td>0.10345</td>\n",
       "      <td>0.35227</td>\n",
       "      <td>16.466</td>\n",
       "      <td>5.272271e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.057030</td>\n",
       "      <td>5.272271e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-31</th>\n",
       "      <td>AEO</td>\n",
       "      <td>0.073315</td>\n",
       "      <td>0.543977</td>\n",
       "      <td>1.736371</td>\n",
       "      <td>1.663056</td>\n",
       "      <td>0.253181</td>\n",
       "      <td>0.316677</td>\n",
       "      <td>8.883611</td>\n",
       "      <td>2.184208</td>\n",
       "      <td>0.026263</td>\n",
       "      <td>...</td>\n",
       "      <td>2.833750</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.12362</td>\n",
       "      <td>0.03903</td>\n",
       "      <td>30.098</td>\n",
       "      <td>4.605641e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.065551</td>\n",
       "      <td>4.605641e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-31</th>\n",
       "      <td>ALGN</td>\n",
       "      <td>-0.201843</td>\n",
       "      <td>-0.489387</td>\n",
       "      <td>-0.582651</td>\n",
       "      <td>-0.380807</td>\n",
       "      <td>0.446573</td>\n",
       "      <td>0.566262</td>\n",
       "      <td>8.660000</td>\n",
       "      <td>2.158715</td>\n",
       "      <td>-0.011416</td>\n",
       "      <td>...</td>\n",
       "      <td>3.036553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.14</td>\n",
       "      <td>0.09577</td>\n",
       "      <td>0.09501</td>\n",
       "      <td>54.387</td>\n",
       "      <td>1.209145e+10</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.125866</td>\n",
       "      <td>1.209145e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-31</th>\n",
       "      <td>AMAT</td>\n",
       "      <td>-0.066902</td>\n",
       "      <td>-0.067995</td>\n",
       "      <td>-0.266944</td>\n",
       "      <td>-0.200042</td>\n",
       "      <td>0.288008</td>\n",
       "      <td>0.323094</td>\n",
       "      <td>11.517730</td>\n",
       "      <td>2.443888</td>\n",
       "      <td>-0.008729</td>\n",
       "      <td>...</td>\n",
       "      <td>11.350218</td>\n",
       "      <td>0.62</td>\n",
       "      <td>8.67</td>\n",
       "      <td>0.35508</td>\n",
       "      <td>0.24669</td>\n",
       "      <td>35.284</td>\n",
       "      <td>2.327789e+11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097484</td>\n",
       "      <td>2.327789e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           symbol    mom_1m    mom_6m   mom_12m  mom_12_1m    vol_3m  \\\n",
       "date                                                                   \n",
       "2005-01-31    AAP -0.007141  0.161412  0.107400   0.114541  0.177014   \n",
       "2005-01-31   ABCB -0.054833  0.176429  0.130269   0.185102  0.303383   \n",
       "2005-01-31    AEO  0.073315  0.543977  1.736371   1.663056  0.253181   \n",
       "2005-01-31   ALGN -0.201843 -0.489387 -0.582651  -0.380807  0.446573   \n",
       "2005-01-31   AMAT -0.066902 -0.067995 -0.266944  -0.200042  0.288008   \n",
       "\n",
       "             vol_12m      price  log_price  returns_1d  ...   pb_ratio  \\\n",
       "date                                                    ...              \n",
       "2005-01-31  0.272406  24.240944   3.188043    0.012926  ...   1.126753   \n",
       "2005-01-31  0.286354  12.617585   2.535091    0.020060  ...   1.307446   \n",
       "2005-01-31  0.316677   8.883611   2.184208    0.026263  ...   2.833750   \n",
       "2005-01-31  0.566262   8.660000   2.158715   -0.011416  ...   3.036553   \n",
       "2005-01-31  0.323094  11.517730   2.443888   -0.008729  ...  11.350218   \n",
       "\n",
       "            dividend_yield    eps      roe  profit_margin  revenue_per_share  \\\n",
       "date                                                                           \n",
       "2005-01-31            2.42 -10.19 -0.23868       -0.04369            144.046   \n",
       "2005-01-31            1.04   5.77  0.10345        0.35227             16.466   \n",
       "2005-01-31            1.84   1.13  0.12362        0.03903             30.098   \n",
       "2005-01-31             NaN   5.14  0.09577        0.09501             54.387   \n",
       "2005-01-31            0.62   8.67  0.35508        0.24669             35.284   \n",
       "\n",
       "              market_cap  covid_period  ret_fwd_1m        mktcap  \n",
       "date                                                              \n",
       "2005-01-31  2.474117e+09             0    0.168909  2.474117e+09  \n",
       "2005-01-31  5.272271e+09             0    0.057030  5.272271e+09  \n",
       "2005-01-31  4.605641e+09             0    0.065551  4.605641e+09  \n",
       "2005-01-31  1.209145e+10             0   -0.125866  1.209145e+10  \n",
       "2005-01-31  2.327789e+11             0    0.097484  2.327789e+11  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load cross-sectional data\n",
    "data_path = repo_root / 'data' / 'cross_sectional' / 'master_table.csv'\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns[:10])}...\")\n",
    "\n",
    "# Set date as index for proper time-series splitting (matching evaluation script)\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.set_index('date')\n",
    "    print(f\"\\nDate range: {df.index.min()} to {df.index.max()}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2005-01-01 to 2017-12-31\n",
      "Validation period: 2018-01-01 to 2019-12-31\n",
      "Train set: 18826 samples\n",
      "Validation set: 3408 samples\n",
      "\n",
      "Number of features: 22\n",
      "Target column: returns_1d\n",
      "\n",
      "Train features shape: (18826, 22)\n",
      "Validation features shape: (3408, 22)\n",
      "Feature columns: ['mom_1m', 'mom_6m', 'mom_12m', 'mom_12_1m', 'vol_3m']... (22 total)\n"
     ]
    }
   ],
   "source": [
    "# Use the same CrossSectionalDataSplitter as the evaluation script\n",
    "class CrossSectionalDataSplitter:\n",
    "    \"\"\"Simple data splitter for cross-sectional data (matching evaluation script).\"\"\"\n",
    "    \n",
    "    def __init__(self, train_start='2005-01-01', train_end='2017-12-31',\n",
    "                 val_start='2018-01-01', val_end='2019-12-31'):\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.val_start = val_start\n",
    "        self.val_end = val_end\n",
    "    \n",
    "    def split(self, master_table):\n",
    "        \"\"\"Split data into train and validation sets.\"\"\"\n",
    "        master_table.index = pd.to_datetime(master_table.index)\n",
    "        \n",
    "        train_data = master_table.loc[self.train_start:self.train_end]\n",
    "        val_data = master_table.loc[self.val_start:self.val_end]\n",
    "        \n",
    "        return {\n",
    "            'train': train_data,\n",
    "            'val': val_data\n",
    "        }\n",
    "    \n",
    "    def prepare_features_labels(self, data):\n",
    "        \"\"\"Prepare features and labels from data (matching evaluation script logic).\"\"\"\n",
    "        if data.empty:\n",
    "            return pd.DataFrame(), pd.Series()\n",
    "        \n",
    "        numeric_data = data.select_dtypes(include=[np.number])\n",
    "        \n",
    "        if numeric_data.empty:\n",
    "            print(\"Warning: No numeric columns found in data\")\n",
    "            return pd.DataFrame(), pd.Series()\n",
    "        \n",
    "        exclude_cols = ['mktcap', 'market_cap', 'date', 'year', 'month', 'ticker', 'permno', 'gvkey']\n",
    "        target_cols = ['return', 'returns', 'ret', 'target', 'y', 'next_return', 'forward_return', \n",
    "                      'ret_1', 'ret_1m', 'ret_12m', 'future_return', 'returns_1d']\n",
    "        \n",
    "        target_col = None\n",
    "        for tc in target_cols:\n",
    "            for col in numeric_data.columns:\n",
    "                if tc.lower() in col.lower() and col.lower() not in [ec.lower() for ec in exclude_cols]:\n",
    "                    target_col = col\n",
    "                    break\n",
    "            if target_col:\n",
    "                break\n",
    "        \n",
    "        if target_col is None:\n",
    "            potential_targets = [col for col in numeric_data.columns \n",
    "                               if col.lower() not in [ec.lower() for ec in exclude_cols]]\n",
    "            if potential_targets:\n",
    "                target_col = potential_targets[-2] if len(potential_targets) > 1 else potential_targets[-1]\n",
    "            else:\n",
    "                target_col = numeric_data.columns[-1]\n",
    "        \n",
    "        feature_cols = [col for col in numeric_data.columns \n",
    "                       if col != target_col and col.lower() not in [ec.lower() for ec in exclude_cols]]\n",
    "        \n",
    "        if not feature_cols:\n",
    "            feature_cols = [col for col in numeric_data.columns if col != target_col]\n",
    "        \n",
    "        if not feature_cols:\n",
    "            feature_cols = numeric_data.columns[:-1].tolist()\n",
    "            target_col = numeric_data.columns[-1]\n",
    "        \n",
    "        X = numeric_data[feature_cols]\n",
    "        y = numeric_data[target_col]\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "# Initialize splitter and split data\n",
    "splitter = CrossSectionalDataSplitter()\n",
    "data_splits = splitter.split(df)\n",
    "\n",
    "train_df = data_splits['train']\n",
    "val_df = data_splits['val']\n",
    "\n",
    "print(f\"Train period: {splitter.train_start} to {splitter.train_end}\")\n",
    "print(f\"Validation period: {splitter.val_start} to {splitter.val_end}\")\n",
    "print(f\"Train set: {train_df.shape[0]} samples\")\n",
    "print(f\"Validation set: {val_df.shape[0]} samples\")\n",
    "\n",
    "# Prepare features and labels using the same logic as evaluation script\n",
    "X_train_df, y_train = splitter.prepare_features_labels(train_df)\n",
    "X_val_df, y_val = splitter.prepare_features_labels(val_df)\n",
    "\n",
    "print(f\"\\nNumber of features: {X_train_df.shape[1]}\")\n",
    "print(f\"Target column: {y_train.name}\")\n",
    "\n",
    "# Fill NaN values and convert to numpy arrays\n",
    "X_train = X_train_df.fillna(0).values.astype(np.float32)\n",
    "y_train = y_train.fillna(0).values.astype(np.float32)\n",
    "X_val = X_val_df.fillna(0).values.astype(np.float32)\n",
    "y_val = y_val.fillna(0).values.astype(np.float32)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(f\"\\nTrain features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Validation features shape: {X_val_scaled.shape}\")\n",
    "print(f\"Feature columns: {list(X_train_df.columns[:5])}... ({len(X_train_df.columns)} total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Models\n",
    "\n",
    "Train all models from scratch on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Store trained models\n",
    "models = {}\n",
    "training_history = {}\n",
    "\n",
    "# Training hyperparameters\n",
    "TRAINING_CONFIG = {\n",
    "    'ols': {},\n",
    "    'ridge': {'alpha': 1.0},\n",
    "    'mlp': {\n",
    "        'hidden_dims': [128, 64],\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 64,\n",
    "        'epochs': 100,\n",
    "        'patience': 10\n",
    "    },\n",
    "    'transformer': {\n",
    "        'd_model': 72,\n",
    "        'num_heads': 8,  \n",
    "        'num_layers': 2,\n",
    "        'd_ff': 512,\n",
    "        'dropout': 0.1,\n",
    "        'learning_rate': 0.0001,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 100,\n",
    "        'patience': 20\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configurationdevice = 'cuda' if torch.cuda.is_available() else 'cpu'print(f\"Using device: {device}\")# Store trained modelsmodels = {}training_history = {}# Training hyperparametersTRAINING_CONFIG = {    'ols': {},    'ridge': {'alpha': 1.0},    'mlp': {        'hidden_dims': [128, 64],        'learning_rate': 0.001,        'batch_size': 64,        'epochs': 50,        'patience': 10    },    'transformer': {        'd_model': 64,        'num_heads': 6,        'num_layers': 2,        'd_ff': 512,        'dropout': 0.1,        'learning_rate': 0.0001,        'batch_size': 32,        'epochs': 50,        'patience': 20    }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING OLS MODEL\n",
      "================================================================================\n",
      "✓ OLS trained\n",
      "  Validation RMSE: 0.017520\n",
      "  Validation R²: -0.007658\n"
     ]
    }
   ],
   "source": [
    "# Train OLS (Linear Regression)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING OLS MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ols_model = LinearRegression()\n",
    "ols_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "models['OLS'] = ols_model\n",
    "\n",
    "# Evaluate on validation set\n",
    "ols_pred = ols_model.predict(X_val_scaled)\n",
    "ols_rmse = np.sqrt(mean_squared_error(y_val, ols_pred))\n",
    "ols_r2 = r2_score(y_val, ols_pred)\n",
    "\n",
    "print(f\"✓ OLS trained\")\n",
    "print(f\"  Validation RMSE: {ols_rmse:.6f}\")\n",
    "print(f\"  Validation R²: {ols_r2:.6f}\")\n",
    "\n",
    "training_history['OLS'] = {'rmse': ols_rmse, 'r2': ols_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING RIDGE MODEL\n",
      "================================================================================\n",
      "✓ Ridge trained\n",
      "  Validation RMSE: 0.017519\n",
      "  Validation R²: -0.007585\n"
     ]
    }
   ],
   "source": [
    "# Train Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING RIDGE MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ridge_model = Ridge(alpha=TRAINING_CONFIG['ridge']['alpha'], random_state=RANDOM_SEED)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "models['Ridge'] = ridge_model\n",
    "\n",
    "# Evaluate on validation set\n",
    "ridge_pred = ridge_model.predict(X_val_scaled)\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_val, ridge_pred))\n",
    "ridge_r2 = r2_score(y_val, ridge_pred)\n",
    "\n",
    "print(f\"✓ Ridge trained\")\n",
    "print(f\"  Validation RMSE: {ridge_rmse:.6f}\")\n",
    "print(f\"  Validation R²: {ridge_r2:.6f}\")\n",
    "\n",
    "training_history['Ridge'] = {'rmse': ridge_rmse, 'r2': ridge_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING MLP MODEL\n",
      "================================================================================\n",
      "  Epoch 10/100: Train Loss=0.003232, Val Loss=0.001477\n",
      "  Epoch 20/100: Train Loss=0.002140, Val Loss=0.000649\n",
      "  Epoch 30/100: Train Loss=0.001559, Val Loss=0.000554\n",
      "  Epoch 40/100: Train Loss=0.001308, Val Loss=0.000499\n",
      "  Epoch 50/100: Train Loss=0.001119, Val Loss=0.000447\n",
      "  Epoch 60/100: Train Loss=0.001020, Val Loss=0.000411\n",
      "  Epoch 70/100: Train Loss=0.000936, Val Loss=0.000399\n",
      "  Epoch 80/100: Train Loss=0.000861, Val Loss=0.000381\n",
      "  Epoch 90/100: Train Loss=0.000810, Val Loss=0.000374\n",
      "  Epoch 100/100: Train Loss=0.000769, Val Loss=0.000367\n",
      "✓ MLP trained\n",
      "  Validation RMSE: 0.019169\n",
      "  Validation R²: -0.206334\n"
     ]
    }
   ],
   "source": [
    "# Train MLP (Multi-Layer Perceptron)\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING MLP MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple MLP for regression.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(-1)\n",
    "\n",
    "mlp_config = TRAINING_CONFIG['mlp']\n",
    "mlp_model = MLP(\n",
    "    input_dim=X_train_scaled.shape[1],\n",
    "    hidden_dims=mlp_config['hidden_dims'],\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=mlp_config['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "\n",
    "for epoch in range(mlp_config['epochs']):\n",
    "    # Training\n",
    "    mlp_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_pred = mlp_model(X_train_tensor)\n",
    "    train_loss = criterion(train_pred, y_train_tensor)\n",
    "    train_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(mlp_model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    mlp_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = mlp_model(X_val_tensor)\n",
    "        val_loss = criterion(val_pred, y_val_tensor)\n",
    "    \n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= mlp_config['patience']:\n",
    "            print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"  Epoch {epoch+1}/{mlp_config['epochs']}: Train Loss={train_loss.item():.6f}, Val Loss={val_loss.item():.6f}\")\n",
    "\n",
    "mlp_model.eval()\n",
    "with torch.no_grad():\n",
    "    mlp_pred = mlp_model(X_val_tensor).cpu().numpy()\n",
    "\n",
    "mlp_rmse = np.sqrt(mean_squared_error(y_val, mlp_pred))\n",
    "mlp_r2 = r2_score(y_val, mlp_pred)\n",
    "\n",
    "models['MLP'] = mlp_model\n",
    "print(f\"✓ MLP trained\")\n",
    "print(f\"  Validation RMSE: {mlp_rmse:.6f}\")\n",
    "print(f\"  Validation R²: {mlp_r2:.6f}\")\n",
    "\n",
    "training_history['MLP'] = {'rmse': mlp_rmse, 'r2': mlp_r2, 'train_losses': train_losses, 'val_losses': val_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING TRANSFORMER MODELS\n",
      "================================================================================\n",
      "\n",
      "Training Single-Head Transformer...\n",
      "  Single-Head - Epoch 10/100: Train Loss=0.000595, Val Loss=0.000295\n",
      "  Single-Head - Epoch 20/100: Train Loss=0.000539, Val Loss=0.000285\n",
      "  Single-Head - Epoch 30/100: Train Loss=0.000506, Val Loss=0.000282\n",
      "  Single-Head - Epoch 40/100: Train Loss=0.000468, Val Loss=0.000279\n",
      "  Single-Head - Epoch 50/100: Train Loss=0.000453, Val Loss=0.000277\n",
      "  Single-Head - Epoch 60/100: Train Loss=0.000448, Val Loss=0.000276\n",
      "  Single-Head - Epoch 70/100: Train Loss=0.000443, Val Loss=0.000276\n",
      "  Single-Head: Early stopping at epoch 79\n",
      "Single-Head trained - RMSE: 0.016650, R²: 0.089910\n"
     ]
    }
   ],
   "source": [
    "# Train Transformer Models\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING TRANSFORMER MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def train_transformer(model, model_name, X_train, y_train, X_val, y_val, config, device='cpu'):\n",
    "    \"\"\"Train a transformer model.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "    \n",
    "    # Handle feature dimension mismatch\n",
    "    num_features = model.num_features if hasattr(model, 'num_features') else model.model.num_features\n",
    "    \n",
    "    if X_train.shape[1] != num_features:\n",
    "        if X_train.shape[1] < num_features:\n",
    "            # Pad\n",
    "            padding_train = np.zeros((X_train.shape[0], num_features - X_train.shape[1]))\n",
    "            padding_val = np.zeros((X_val.shape[0], num_features - X_val.shape[1]))\n",
    "            X_train_tensor = torch.FloatTensor(np.hstack([X_train, padding_train])).to(device)\n",
    "            X_val_tensor = torch.FloatTensor(np.hstack([X_val, padding_val])).to(device)\n",
    "        else:\n",
    "            # Truncate\n",
    "            X_train_tensor = torch.FloatTensor(X_train[:, :num_features]).to(device)\n",
    "            X_val_tensor = torch.FloatTensor(X_val[:, :num_features]).to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    batch_size = config['batch_size']\n",
    "    n_batches = (len(X_train_tensor) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        \n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            batch_X = X_train_tensor[i:i+batch_size]\n",
    "            batch_y = y_train_tensor[i:i+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch_X)\n",
    "            if isinstance(pred, tuple):\n",
    "                pred = pred[0]\n",
    "            loss = criterion(pred.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "        \n",
    "        epoch_train_loss /= n_batches\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            if isinstance(val_pred, tuple):\n",
    "                val_pred = val_pred[0]\n",
    "            val_loss = criterion(val_pred.squeeze(), y_val_tensor)\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(val_loss.item())\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config['patience']:\n",
    "                print(f\"  {model_name}: Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  {model_name} - Epoch {epoch+1}/{config['epochs']}: Train Loss={epoch_train_loss:.6f}, Val Loss={val_loss.item():.6f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_pred = model(X_val_tensor)\n",
    "        if isinstance(final_pred, tuple):\n",
    "            final_pred = final_pred[0]\n",
    "        final_pred = final_pred.squeeze().cpu().numpy()\n",
    "    \n",
    "    return model, final_pred, train_losses, val_losses\n",
    "\n",
    "# Train Single-Head Transformer\n",
    "print(\"\\nTraining Single-Head Transformer...\")\n",
    "single_head_model = SingleHeadTransformer(\n",
    "    num_features=X_train_scaled.shape[1],\n",
    "    d_model=TRAINING_CONFIG['transformer']['d_model'],\n",
    "    num_layers=TRAINING_CONFIG['transformer']['num_layers']\n",
    ")\n",
    "single_head_model, single_head_pred, sh_train_losses, sh_val_losses = train_transformer(\n",
    "    single_head_model, 'Single-Head', X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "    TRAINING_CONFIG['transformer'], device\n",
    ")\n",
    "\n",
    "single_head_rmse = np.sqrt(mean_squared_error(y_val, single_head_pred))\n",
    "single_head_r2 = r2_score(y_val, single_head_pred)\n",
    "\n",
    "models['Single-Head'] = single_head_model\n",
    "print(f\"Single-Head trained - RMSE: {single_head_rmse:.6f}, R²: {single_head_r2:.6f}\")\n",
    "training_history['Single-Head'] = {'rmse': single_head_rmse, 'r2': single_head_r2, 'train_losses': sh_train_losses, 'val_losses': sh_val_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Multi-Head Transformer...\n",
      "  Multi-Head - Epoch 10/100: Train Loss=0.000608, Val Loss=0.000302\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train Multi-Head Transformer\n",
    "print(\"\\nTraining Multi-Head Transformer...\")\n",
    "multi_head_model = FeatureTokenTransformer(\n",
    "    num_features=X_train_scaled.shape[1],\n",
    "    d_model=TRAINING_CONFIG['transformer']['d_model'],\n",
    "    num_heads=TRAINING_CONFIG['transformer']['num_heads'],\n",
    "    num_layers=TRAINING_CONFIG['transformer']['num_layers'],\n",
    "    d_ff=TRAINING_CONFIG['transformer']['d_ff'],\n",
    "    dropout=TRAINING_CONFIG['transformer']['dropout'],\n",
    "    use_head_diversity=False\n",
    ")\n",
    "multi_head_model, multi_head_pred, mh_train_losses, mh_val_losses = train_transformer(\n",
    "    multi_head_model, 'Multi-Head', X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "    TRAINING_CONFIG['transformer'], device\n",
    ")\n",
    "\n",
    "multi_head_rmse = np.sqrt(mean_squared_error(y_val, multi_head_pred))\n",
    "multi_head_r2 = r2_score(y_val, multi_head_pred)\n",
    "\n",
    "models['Multi-Head'] = multi_head_model\n",
    "print(f\"✓ Multi-Head trained - RMSE: {multi_head_rmse:.6f}, R²: {multi_head_r2:.6f}\")\n",
    "training_history['Multi-Head'] = {'rmse': multi_head_rmse, 'r2': multi_head_r2, 'train_losses': mh_train_losses, 'val_losses': mh_val_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multi-Head Diversity Transformer\n",
    "print(\"\\nTraining Multi-Head Diversity Transformer...\")\n",
    "multi_head_diversity_model = FeatureTokenTransformer(\n",
    "    num_features=X_train_scaled.shape[1],\n",
    "    d_model=TRAINING_CONFIG['transformer']['d_model'],\n",
    "    num_heads=TRAINING_CONFIG['transformer']['num_heads'],\n",
    "    num_layers=TRAINING_CONFIG['transformer']['num_layers'],\n",
    "    d_ff=TRAINING_CONFIG['transformer']['d_ff'],\n",
    "    dropout=TRAINING_CONFIG['transformer']['dropout'],\n",
    "    use_head_diversity=True,\n",
    "    diversity_weight=0.01\n",
    ")\n",
    "multi_head_diversity_model, mhd_pred, mhd_train_losses, mhd_val_losses = train_transformer(\n",
    "    multi_head_diversity_model, 'Multi-Head Diversity', X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "    TRAINING_CONFIG['transformer'], device\n",
    ")\n",
    "\n",
    "multi_head_diversity_rmse = np.sqrt(mean_squared_error(y_val, mhd_pred))\n",
    "multi_head_diversity_r2 = r2_score(y_val, mhd_pred)\n",
    "\n",
    "models['Multi-Head Diversity'] = multi_head_diversity_model\n",
    "print(f\"✓ Multi-Head Diversity trained - RMSE: {multi_head_diversity_rmse:.6f}, R²: {multi_head_diversity_r2:.6f}\")\n",
    "training_history['Multi-Head Diversity'] = {'rmse': multi_head_diversity_rmse, 'r2': multi_head_diversity_r2, 'train_losses': mhd_train_losses, 'val_losses': mhd_val_losses}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL MODELS TRAINED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total models trained: {len(models)}\")\n",
    "print(\"\\nValidation Results Summary:\")\n",
    "for name, metrics in training_history.items():\n",
    "    print(f\"  {name}: RMSE={metrics['rmse']:.6f}, R²={metrics['r2']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, X, device='cpu', batch_size=128, is_sklearn=False):\n",
    "    \"\"\"Make predictions with a model (deterministic mode).\"\"\"\n",
    "    if is_sklearn:\n",
    "        # sklearn models\n",
    "        return model.predict(X)\n",
    "    \n",
    "    # PyTorch models\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure deterministic mode - disable dropout\n",
    "    with torch.no_grad():\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.eval()\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # Get num_features - handle both FeatureTokenTransformer and SingleHeadTransformer\n",
    "        if hasattr(model, 'num_features'):\n",
    "            num_features = model.num_features\n",
    "        elif hasattr(model, 'model') and hasattr(model.model, 'num_features'):\n",
    "            # SingleHeadTransformer wraps FeatureTokenTransformer in self.model\n",
    "            num_features = model.model.num_features\n",
    "        else:\n",
    "            # Fallback: use input dimension\n",
    "            num_features = X.shape[1]\n",
    "        \n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = X[i:i+batch_size]\n",
    "            X_tensor = torch.FloatTensor(batch).to(device)\n",
    "            \n",
    "            # Handle padding if needed\n",
    "            if X_tensor.shape[1] != num_features:\n",
    "                if X_tensor.shape[1] < num_features:\n",
    "                    padding = torch.zeros(X_tensor.shape[0], num_features - X_tensor.shape[1]).to(device)\n",
    "                    X_tensor = torch.cat([X_tensor, padding], dim=1)\n",
    "                else:\n",
    "                    X_tensor = X_tensor[:, :num_features]\n",
    "            \n",
    "            pred = model(X_tensor)\n",
    "            if isinstance(pred, tuple):\n",
    "                pred = pred[0]\n",
    "            predictions.append(pred.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(predictions, axis=0).flatten()\n",
    "\n",
    "# Make predictions on validation set\n",
    "print(\"Making predictions on validation set...\")\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    is_sklearn = name in ['OLS', 'Ridge']\n",
    "    pred = make_predictions(model, X_val_scaled, device, is_sklearn=is_sklearn)\n",
    "    predictions[name] = pred.flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    r2 = r2_score(y_val, pred)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  RMSE: {rmse:.6f}\")\n",
    "    print(f\"  R²: {r2:.6f}\")\n",
    "    print()\n",
    "comparison_data = []\n",
    "for name in ['OLS', 'Ridge', 'MLP', 'Single-Head', 'Multi-Head', 'Multi-Head Diversity']:\n",
    "    if name in training_history:\n",
    "        comparison_data.append({\n",
    "            'Model': name,\n",
    "            'RMSE': training_history[name]['rmse'],\n",
    "            'R²': training_history[name]['r2']\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('R²', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "ax1.barh(comparison_df['Model'], comparison_df['RMSE'])\n",
    "ax1.set_xlabel('RMSE', fontsize=12)\n",
    "ax1.set_title('RMSE Comparison (Lower is Better)', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# R² comparison\n",
    "ax2.barh(comparison_df['Model'], comparison_df['R²'])\n",
    "ax2.set_xlabel('R²', fontsize=12)\n",
    "ax2.set_title('R² Comparison (Higher is Better)', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adversarial Training (A1-A4 Attacks)\n",
    "\n",
    "Train transformer models with adversarial training against A1-A4 attacks to improve robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial Attack Implementations (A1-A4)\n",
    "def apply_a1_attack(X, epsilon=0.01):\n",
    "    \"\"\"A1: Measurement Error - bounded perturbations.\"\"\"\n",
    "    noise = np.random.normal(0, epsilon, X.shape)\n",
    "    # Scale noise by feature standard deviation\n",
    "    feature_std = np.std(X, axis=0, keepdims=True) + 1e-8\n",
    "    noise = noise * feature_std\n",
    "    return X + noise\n",
    "\n",
    "\n",
    "def apply_a2_attack(X, missing_rate=0.1):\n",
    "    \"\"\"A2: Missingness/Staleness - set random features to zero.\"\"\"\n",
    "    X_adv = X.copy()\n",
    "    n_samples, n_features = X.shape\n",
    "    n_missing = int(n_features * missing_rate)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        missing_indices = np.random.choice(n_features, n_missing, replace=False)\n",
    "        X_adv[i, missing_indices] = 0.0\n",
    "    \n",
    "    return X_adv\n",
    "\n",
    "\n",
    "def apply_a3_attack(X, epsilon=0.01):\n",
    "    \"\"\"A3: Rank Manipulation - cross-sectional perturbation preserving ranks.\"\"\"\n",
    "    X_adv = X.copy()\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Add small random perturbation that preserves relative ordering\n",
    "    for i in range(n_samples):\n",
    "        perturbation = np.random.normal(0, epsilon, X.shape[1])\n",
    "        # Scale by feature std to maintain relative magnitudes\n",
    "        feature_std = np.std(X[i], axis=0) + 1e-8\n",
    "        perturbation = perturbation * feature_std\n",
    "        X_adv[i] = X[i] + perturbation\n",
    "    \n",
    "    return X_adv\n",
    "\n",
    "\n",
    "def apply_a4_attack(X, epsilon=1.0):\n",
    "    \"\"\"A4: Regime Shift - distribution shift attack.\"\"\"\n",
    "    # A4 simulates regime shift by scaling volatility\n",
    "    # epsilon acts as volatility multiplier\n",
    "    X_adv = X.copy()\n",
    "    feature_std = np.std(X, axis=0, keepdims=True) + 1e-8\n",
    "    # Generate noise with std = epsilon, then scale by feature std\n",
    "    noise = np.random.normal(0, epsilon, X.shape) * feature_std\n",
    "    X_adv = X + noise\n",
    "    return X_adv\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial Training Configuration\n",
    "ADVERSARIAL_CONFIG = {\n",
    "    'epsilons': [0.25, 0.5, 1.0],  # Attack strengths\n",
    "    'attacks': ['a1', 'a2', 'a3', 'a4'],  # Attack types\n",
    "    'robust_weight': 0.3,  # Weight for adversarial loss (0.3 = 30% adversarial, 70% clean)\n",
    "    'learning_rate': 0.0001,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100,\n",
    "    'patience': 20,\n",
    "    'warmup_epochs': 5  # Gradually increase adversarial weight\n",
    "}\n",
    "\n",
    "# Store adversarially trained models\n",
    "adversarial_models = {}\n",
    "adversarial_training_history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training_step(model, X_batch, y_batch, attack_type, epsilon, \n",
    "                             optimizer, device='cpu', robust_weight=0.3):\n",
    "    \"\"\"\n",
    "    Perform one adversarial training step.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        X_batch: Input batch (numpy array)\n",
    "        y_batch: Target batch (numpy array)\n",
    "        attack_type: 'a1', 'a2', 'a3', or 'a4'\n",
    "        epsilon: Attack strength\n",
    "        optimizer: Optimizer\n",
    "        device: Device to use\n",
    "        robust_weight: Weight for adversarial loss\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with loss values or None if batch is invalid\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_tensor = torch.FloatTensor(X_batch).to(device)\n",
    "    y_tensor = torch.FloatTensor(y_batch).to(device)\n",
    "    \n",
    "    # Clean forward pass\n",
    "    output_clean = model(X_tensor)\n",
    "    if isinstance(output_clean, tuple):\n",
    "        y_pred_clean = output_clean[0]\n",
    "    else:\n",
    "        y_pred_clean = output_clean\n",
    "    \n",
    "    # Check for NaN/Inf in predictions\n",
    "    if torch.any(torch.isnan(y_pred_clean)) or torch.any(torch.isinf(y_pred_clean)):\n",
    "        return None\n",
    "    \n",
    "    clean_loss = nn.MSELoss()(y_pred_clean.squeeze(), y_tensor)\n",
    "    \n",
    "    # Check if clean_loss is valid\n",
    "    if torch.isnan(clean_loss) or torch.isinf(clean_loss):\n",
    "        return None\n",
    "    \n",
    "    # Generate adversarial examples\n",
    "    if attack_type == 'a1':\n",
    "        X_adv = apply_a1_attack(X_batch, epsilon=epsilon)\n",
    "    elif attack_type == 'a2':\n",
    "        # For A2, epsilon controls missing rate\n",
    "        missing_rate = min(epsilon / 10.0, 0.8)  # Convert epsilon to missing rate\n",
    "        X_adv = apply_a2_attack(X_batch, missing_rate=missing_rate)\n",
    "    elif attack_type == 'a3':\n",
    "        X_adv = apply_a3_attack(X_batch, epsilon=epsilon)\n",
    "    elif attack_type == 'a4':\n",
    "        X_adv = apply_a4_attack(X_batch, epsilon=epsilon)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attack type: {attack_type}\")\n",
    "    \n",
    "    # Adversarial forward pass\n",
    "    X_adv_tensor = torch.FloatTensor(X_adv).to(device)\n",
    "    output_adv = model(X_adv_tensor)\n",
    "    if isinstance(output_adv, tuple):\n",
    "        y_pred_adv = output_adv[0]\n",
    "    else:\n",
    "        y_pred_adv = output_adv\n",
    "    \n",
    "    # Check for NaN/Inf in adversarial predictions\n",
    "    if torch.any(torch.isnan(y_pred_adv)) or torch.any(torch.isinf(y_pred_adv)):\n",
    "        return None\n",
    "    \n",
    "    adv_loss = nn.MSELoss()(y_pred_adv.squeeze(), y_tensor)\n",
    "    \n",
    "    # Check if adv_loss is valid\n",
    "    if torch.isnan(adv_loss) or torch.isinf(adv_loss):\n",
    "        return None\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = (1 - robust_weight) * clean_loss + robust_weight * adv_loss\n",
    "    \n",
    "    # Check if total_loss is valid before backward pass\n",
    "    if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "        return None\n",
    "    \n",
    "    # Ensure total_loss requires gradients\n",
    "    if not total_loss.requires_grad:\n",
    "        return None\n",
    "    \n",
    "    # Backward pass with error handling\n",
    "    try:\n",
    "        total_loss.backward()\n",
    "    except RuntimeError as e:\n",
    "        if \"does not require grad\" in str(e) or \"does not have a grad_fn\" in str(e):\n",
    "            optimizer.zero_grad()\n",
    "            return None\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # Gradient clipping to prevent exploding gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return {\n",
    "        'clean_loss': clean_loss.item(),\n",
    "        'adversarial_loss': adv_loss.item(),\n",
    "        'total_loss': total_loss.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_model(model, model_name, X_train, y_train, X_val, y_val, \n",
    "                           attack_type, epsilon, config, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train model with adversarial training.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to train (will be copied)\n",
    "        model_name: Name of the model\n",
    "        X_train: Training features\n",
    "        y_train: Training targets\n",
    "        X_val: Validation features\n",
    "        y_val: Validation targets\n",
    "        attack_type: 'a1', 'a2', 'a3', or 'a4'\n",
    "        epsilon: Attack strength\n",
    "        config: Training configuration\n",
    "        device: Device to use\n",
    "    \n",
    "    Returns:\n",
    "        Trained model, predictions, and training history\n",
    "    \"\"\"\n",
    "    # Create a fresh copy of the model for adversarial training\n",
    "    import copy\n",
    "    model = copy.deepcopy(model)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "    \n",
    "    # Handle feature dimension mismatch\n",
    "    num_features = model.num_features if hasattr(model, 'num_features') else model.model.num_features\n",
    "    \n",
    "    if X_train.shape[1] != num_features:\n",
    "        if X_train.shape[1] < num_features:\n",
    "            # Pad\n",
    "            padding_train = np.zeros((X_train.shape[0], num_features - X_train.shape[1]))\n",
    "            padding_val = np.zeros((X_val.shape[0], num_features - X_val.shape[1]))\n",
    "            X_train_tensor = torch.FloatTensor(np.hstack([X_train, padding_train])).to(device)\n",
    "            X_val_tensor = torch.FloatTensor(np.hstack([X_val, padding_val])).to(device)\n",
    "        else:\n",
    "            # Truncate\n",
    "            X_train_tensor = torch.FloatTensor(X_train[:, :num_features]).to(device)\n",
    "            X_val_tensor = torch.FloatTensor(X_val[:, :num_features]).to(device)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_clean_loss': [],\n",
    "        'train_adv_loss': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    warmup_epochs = config.get('warmup_epochs', 5)\n",
    "    \n",
    "    batch_size = config['batch_size']\n",
    "    n_batches = (len(X_train_tensor) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Gradual warmup: increase robust_weight from 0.1 to target value\n",
    "        if epoch < warmup_epochs:\n",
    "            current_robust_weight = 0.1 + (config['robust_weight'] - 0.1) * (epoch / warmup_epochs)\n",
    "        else:\n",
    "            current_robust_weight = config['robust_weight']\n",
    "        \n",
    "        epoch_losses = {'clean': [], 'adv': [], 'total': []}\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            batch_X = X_train_tensor[i:i+batch_size].cpu().numpy()\n",
    "            batch_y = y_train_tensor[i:i+batch_size].cpu().numpy()\n",
    "            \n",
    "            losses = adversarial_training_step(\n",
    "                model, batch_X, batch_y, attack_type, epsilon,\n",
    "                optimizer, device, current_robust_weight\n",
    "            )\n",
    "            \n",
    "            # Skip batch if None (invalid batch)\n",
    "            if losses is None:\n",
    "                continue\n",
    "            \n",
    "            # Check for NaN/Inf in losses\n",
    "            if (np.isnan(losses['total_loss']) or np.isinf(losses['total_loss']) or\n",
    "                np.isnan(losses['clean_loss']) or np.isinf(losses['clean_loss']) or\n",
    "                np.isnan(losses['adversarial_loss']) or np.isinf(losses['adversarial_loss'])):\n",
    "                continue\n",
    "            \n",
    "            epoch_losses['clean'].append(losses['clean_loss'])\n",
    "            epoch_losses['adv'].append(losses['adversarial_loss'])\n",
    "            epoch_losses['total'].append(losses['total_loss'])\n",
    "        \n",
    "        # Skip epoch if all losses are invalid\n",
    "        if len(epoch_losses['total']) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output_val = model(X_val_tensor)\n",
    "            if isinstance(output_val, tuple):\n",
    "                y_pred_val = output_val[0]\n",
    "            else:\n",
    "                y_pred_val = output_val\n",
    "            \n",
    "            # Check for constant predictions (model collapse detection)\n",
    "            y_pred_np = y_pred_val.squeeze().cpu().numpy()\n",
    "            pred_std = np.std(y_pred_np)\n",
    "            \n",
    "            if pred_std < 1e-8:\n",
    "                print(f\"   MODEL COLLAPSE DETECTED at epoch {epoch+1}!\")\n",
    "                break\n",
    "            \n",
    "            val_loss = nn.MSELoss()(y_pred_val.squeeze(), y_val_tensor).item()\n",
    "            \n",
    "            # Check for NaN/Inf in validation loss\n",
    "            if np.isnan(val_loss) or np.isinf(val_loss):\n",
    "                val_loss = float('inf')\n",
    "        \n",
    "        # Record history\n",
    "        avg_train_loss = np.mean(epoch_losses['total']) if epoch_losses['total'] else float('inf')\n",
    "        avg_clean_loss = np.mean(epoch_losses['clean']) if epoch_losses['clean'] else 0.0\n",
    "        avg_adv_loss = np.mean(epoch_losses['adv']) if epoch_losses['adv'] else 0.0\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_clean_loss'].append(avg_clean_loss)\n",
    "        history['train_adv_loss'].append(avg_adv_loss)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if not (np.isnan(val_loss) or np.isinf(val_loss)):\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= config['patience']:\n",
    "                    print(f\"  {model_name} ({attack_type.upper()}, ε={epsilon}): Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  {model_name} ({attack_type.upper()}, ε={epsilon}) - Epoch {epoch+1}/{config['epochs']}: \"\n",
    "                  f\"Train Loss={avg_train_loss:.6f}, Val Loss={val_loss:.6f}, \"\n",
    "                  f\"Robust Weight={current_robust_weight:.3f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_pred = model(X_val_tensor)\n",
    "        if isinstance(final_pred, tuple):\n",
    "            final_pred = final_pred[0]\n",
    "        final_pred = final_pred.squeeze().cpu().numpy()\n",
    "    \n",
    "    return model, final_pred, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train adversarially trained models\n",
    "print(\"=\" * 80)\n",
    "print(\"ADVERSARIAL TRAINING FOR TRANSFORMER MODELS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training on attacks: {ADVERSARIAL_CONFIG['attacks']}\")\n",
    "print(f\"Epsilons: {ADVERSARIAL_CONFIG['epsilons']}\")\n",
    "print(f\"Robust weight: {ADVERSARIAL_CONFIG['robust_weight']}\")\n",
    "print()\n",
    "\n",
    "# Models to train adversarially\n",
    "transformer_model_names = ['Single-Head', 'Multi-Head', 'Multi-Head Diversity']\n",
    "base_models = {\n",
    "    'Single-Head': models['Single-Head'],\n",
    "    'Multi-Head': models['Multi-Head'],\n",
    "    'Multi-Head Diversity': models['Multi-Head Diversity']\n",
    "}\n",
    "\n",
    "# Train each model with each attack at each epsilon\n",
    "for model_name in transformer_model_names:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {model_name} with Adversarial Training\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    base_model = base_models[model_name]\n",
    "    \n",
    "    for attack_type in ADVERSARIAL_CONFIG['attacks']:\n",
    "        for epsilon in ADVERSARIAL_CONFIG['epsilons']:\n",
    "            model_key = f\"{model_name} ({attack_type.upper()}, ε={epsilon})\"\n",
    "            print(f\"\\nTraining {model_key}...\")\n",
    "            \n",
    "            try:\n",
    "                adv_model, adv_pred, adv_history = train_adversarial_model(\n",
    "                    base_model, model_name, X_train_scaled, y_train, \n",
    "                    X_val_scaled, y_val, attack_type, epsilon, \n",
    "                    ADVERSARIAL_CONFIG, device\n",
    "                )\n",
    "                \n",
    "                # Evaluate\n",
    "                adv_rmse = np.sqrt(mean_squared_error(y_val, adv_pred))\n",
    "                adv_r2 = r2_score(y_val, adv_pred)\n",
    "                \n",
    "                adversarial_models[model_key] = adv_model\n",
    "                adversarial_training_history[model_key] = {\n",
    "                    'rmse': adv_rmse,\n",
    "                    'r2': adv_r2,\n",
    "                    'history': adv_history\n",
    "                }\n",
    "                \n",
    "                print(f\"  {model_key} trained - RMSE: {adv_rmse:.6f}, R²: {adv_r2:.6f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" Error training {model_key}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ADVERSARIAL TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total adversarially trained models: {len(adversarial_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Existing Models Under Adversarial Attacks\n",
    "\n",
    "Evaluate already-trained models (standard and adversarially trained) under A1-A4 attacks to generate robustness results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate existing models under adversarial attacks\n",
    "# This generates robustness_results and robustness_df without retraining\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATING EXISTING MODELS UNDER ADVERSARIAL ATTACKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define attack epsilons and types\n",
    "ATTACK_EPSILONS = [0.25, 0.5, 1.0]\n",
    "ATTACK_TYPES = ['a1', 'a2', 'a3', 'a4']\n",
    "\n",
    "# Attack functions (reuse from adversarial training section)\n",
    "def apply_a1_attack(X, epsilon=0.01):\n",
    "    \"\"\"A1: Measurement Error - bounded perturbations.\"\"\"\n",
    "    noise = np.random.normal(0, epsilon, X.shape)\n",
    "    return X + noise\n",
    "\n",
    "def apply_a2_attack(X, missing_rate=0.1):\n",
    "    \"\"\"A2: Missingness/Staleness - set random features to zero.\"\"\"\n",
    "    X_adv = X.copy()\n",
    "    n_samples, n_features = X.shape\n",
    "    n_missing = max(1, int(n_features * missing_rate))\n",
    "    for i in range(n_samples):\n",
    "        missing_indices = np.random.choice(n_features, n_missing, replace=False)\n",
    "        X_adv[i, missing_indices] = 0.0\n",
    "    return X_adv\n",
    "\n",
    "def apply_a3_attack(X, epsilon=0.01):\n",
    "    \"\"\"A3: Rank Manipulation - cross-sectional perturbation preserving ranks.\"\"\"\n",
    "    X_adv = X.copy()\n",
    "    n_samples = X.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        perturbation = np.random.normal(0, epsilon, X.shape[1])\n",
    "        X_adv[i] = X[i] + perturbation\n",
    "    return X_adv\n",
    "\n",
    "def apply_a4_attack(X, epsilon=1.0):\n",
    "    \"\"\"A4: Regime Shift - distribution shift attack.\"\"\"\n",
    "    X_adv = X.copy()\n",
    "    feature_std = np.std(X, axis=0, keepdims=True) + 1e-8\n",
    "    noise = np.random.normal(0, epsilon, X.shape) * feature_std\n",
    "    X_adv = X + noise\n",
    "    return X_adv\n",
    "\n",
    "# Function to evaluate a model under attack\n",
    "def evaluate_model_under_attack(model, model_name, X_val, y_val, attack_type, epsilon, \n",
    "                                device='cpu', is_sklearn=False, num_runs=5):\n",
    "    \"\"\"Evaluate a model under a specific attack.\"\"\"\n",
    "    # Set model to eval mode\n",
    "    if not is_sklearn:\n",
    "        model.eval()\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.eval()\n",
    "    \n",
    "    # Make clean predictions\n",
    "    if is_sklearn:\n",
    "        y_pred_clean = model.predict(X_val)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X_val).to(device)\n",
    "            output = model(X_tensor)\n",
    "            # Handle tuple returns (some models return (predictions, attention_weights))\n",
    "            if isinstance(output, tuple):\n",
    "                y_pred_tensor = output[0]\n",
    "            else:\n",
    "                y_pred_tensor = output\n",
    "            y_pred_clean = y_pred_tensor.cpu().numpy().flatten()\n",
    "    \n",
    "    # Calculate clean RMSE\n",
    "    clean_rmse = np.sqrt(mean_squared_error(y_val, y_pred_clean))\n",
    "    \n",
    "    # Run attack multiple times and average\n",
    "    adv_rmses = []\n",
    "    for run in range(num_runs):\n",
    "        # Apply attack\n",
    "        if attack_type == 'a1':\n",
    "            X_adv = apply_a1_attack(X_val, epsilon=epsilon)\n",
    "        elif attack_type == 'a2':\n",
    "            # Convert epsilon to missing rate\n",
    "            missing_rate = min(epsilon / 10.0, 0.8)\n",
    "            X_adv = apply_a2_attack(X_val, missing_rate=missing_rate)\n",
    "        elif attack_type == 'a3':\n",
    "            X_adv = apply_a3_attack(X_val, epsilon=epsilon)\n",
    "        elif attack_type == 'a4':\n",
    "            X_adv = apply_a4_attack(X_val, epsilon=epsilon)\n",
    "        else:\n",
    "            X_adv = X_val.copy()\n",
    "        \n",
    "        # Make adversarial predictions\n",
    "        if is_sklearn:\n",
    "            y_pred_adv = model.predict(X_adv)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                X_adv_tensor = torch.FloatTensor(X_adv).to(device)\n",
    "                output_adv = model(X_adv_tensor)\n",
    "                # Handle tuple returns (some models return (predictions, attention_weights))\n",
    "                if isinstance(output_adv, tuple):\n",
    "                    y_pred_adv_tensor = output_adv[0]\n",
    "                else:\n",
    "                    y_pred_adv_tensor = output_adv\n",
    "                y_pred_adv = y_pred_adv_tensor.cpu().numpy().flatten()\n",
    "        \n",
    "        # Calculate adversarial RMSE\n",
    "        adv_rmse = np.sqrt(mean_squared_error(y_val, y_pred_adv))\n",
    "        adv_rmses.append(adv_rmse)\n",
    "    \n",
    "    # Average across runs\n",
    "    avg_adv_rmse = np.mean(adv_rmses)\n",
    "    delta_rmse = avg_adv_rmse - clean_rmse\n",
    "    \n",
    "    # Calculate robustness: min(1.0, 1 - (ΔRMSE / RMSE_clean))\n",
    "    if clean_rmse > 0:\n",
    "        robustness = min(1.0, 1.0 - (delta_rmse / clean_rmse))\n",
    "    else:\n",
    "        robustness = 1.0\n",
    "    \n",
    "    return {\n",
    "        'clean_rmse': clean_rmse,\n",
    "        'adv_rmse': avg_adv_rmse,\n",
    "        'delta_rmse': delta_rmse,\n",
    "        'robustness': robustness\n",
    "    }\n",
    "\n",
    "# Initialize results list\n",
    "robustness_results = []\n",
    "\n",
    "# Check what models are available\n",
    "print(\"\\nAvailable models:\")\n",
    "if 'models' in locals():\n",
    "    print(f\"  Standard models: {list(models.keys())}\")\n",
    "else:\n",
    "    print(\"  ⚠ No standard models found\")\n",
    "    models = {}\n",
    "\n",
    "if 'adversarial_models' in locals():\n",
    "    print(f\"  Adversarially trained models: {len(adversarial_models)} models\")\n",
    "else:\n",
    "    print(\"  ⚠ No adversarially trained models found\")\n",
    "    adversarial_models = {}\n",
    "\n",
    "# Ensure we have validation data\n",
    "if 'X_val_scaled' not in locals() or 'y_val' not in locals():\n",
    "    print(\"\\n⚠ X_val_scaled or y_val not found. Please run data loading and splitting cells first.\")\n",
    "    robustness_df = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"\\nValidation set: {len(X_val_scaled)} samples, {X_val_scaled.shape[1]} features\")\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "    \n",
    "    # Evaluate standard models\n",
    "    print(\"Evaluating standard models...\")\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"  {model_name}...\")\n",
    "        is_sklearn = model_name in ['OLS', 'Ridge']\n",
    "        \n",
    "        for attack_type in ATTACK_TYPES:\n",
    "            for epsilon in ATTACK_EPSILONS:\n",
    "                try:\n",
    "                    result = evaluate_model_under_attack(\n",
    "                        model, model_name, X_val_scaled, y_val.values if hasattr(y_val, 'values') else y_val,\n",
    "                        attack_type, epsilon, device=device, is_sklearn=is_sklearn, num_runs=5\n",
    "                    )\n",
    "                    robustness_results.append({\n",
    "                        'model_name': model_name,\n",
    "                        'attack_type': attack_type,\n",
    "                        'epsilon': epsilon,\n",
    "                        'clean_rmse': result['clean_rmse'],\n",
    "                        'adv_rmse': result['adv_rmse'],\n",
    "                        'delta_rmse': result['delta_rmse'],\n",
    "                        'robustness': result['robustness'],\n",
    "                        'training_type': 'standard'\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"    ⚠ Error evaluating {model_name} under {attack_type} (ε={epsilon}): {e}\")\n",
    "    \n",
    "    # Evaluate adversarially trained models\n",
    "    if len(adversarial_models) > 0:\n",
    "        print(\"\\nEvaluating adversarially trained models...\")\n",
    "        for model_key, adv_model in adversarial_models.items():\n",
    "            # Extract base model name and attack info from key\n",
    "            # Format: \"Multi-Head Diversity (A1, ε=0.25)\"\n",
    "            base_model = model_key.split('(')[0].strip()\n",
    "            print(f\"  {model_key}...\")\n",
    "            \n",
    "            for attack_type in ATTACK_TYPES:\n",
    "                for epsilon in ATTACK_EPSILONS:\n",
    "                    try:\n",
    "                        result = evaluate_model_under_attack(\n",
    "                            adv_model, model_key, X_val_scaled, y_val.values if hasattr(y_val, 'values') else y_val,\n",
    "                            attack_type, epsilon, device=device, is_sklearn=False, num_runs=5\n",
    "                        )\n",
    "                        robustness_results.append({\n",
    "                            'model_name': model_key,\n",
    "                            'attack_type': attack_type,\n",
    "                            'epsilon': epsilon,\n",
    "                            'clean_rmse': result['clean_rmse'],\n",
    "                            'adv_rmse': result['adv_rmse'],\n",
    "                            'delta_rmse': result['delta_rmse'],\n",
    "                            'robustness': result['robustness'],\n",
    "                            'training_type': 'adversarial'\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ⚠ Error evaluating {model_key} under {attack_type} (ε={epsilon}): {e}\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if len(robustness_results) > 0:\n",
    "        robustness_df = pd.DataFrame(robustness_results)\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"EVALUATION COMPLETE\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\n✓ Generated {len(robustness_results)} robustness evaluations\")\n",
    "        print(f\"✓ Created robustness_df with shape: {robustness_df.shape}\")\n",
    "        print(f\"\\nSummary by model:\")\n",
    "        for model_name in robustness_df['model_name'].unique():\n",
    "            model_data = robustness_df[robustness_df['model_name'] == model_name]\n",
    "            avg_robustness = model_data['robustness'].mean()\n",
    "            print(f\"  {model_name}: Average Robustness = {avg_robustness:.4f}\")\n",
    "    else:\n",
    "        print(\"\\n⚠ No robustness results generated. Check that models are available.\")\n",
    "        robustness_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Robustness Results\n",
    "\n",
    "Display a summary table of the robustness evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of robustness results\n",
    "if 'robustness_df' in locals() and len(robustness_df) > 0:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ROBUSTNESS RESULTS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Summary by model\n",
    "    print(\"\\n1. Average Robustness by Model:\")\n",
    "    model_summary = robustness_df.groupby('model_name')['robustness'].agg(['mean', 'std', 'min', 'max']).round(4)\n",
    "    print(model_summary)\n",
    "    \n",
    "    # Summary by attack type\n",
    "    print(\"\\n2. Average Robustness by Attack Type:\")\n",
    "    attack_summary = robustness_df.groupby('attack_type')['robustness'].agg(['mean', 'std', 'min', 'max']).round(4)\n",
    "    print(attack_summary)\n",
    "    \n",
    "    # Summary by epsilon\n",
    "    print(\"\\n3. Average Robustness by Epsilon:\")\n",
    "    epsilon_summary = robustness_df.groupby('epsilon')['robustness'].agg(['mean', 'std', 'min', 'max']).round(4)\n",
    "    print(epsilon_summary)\n",
    "    \n",
    "    # Models with robustness >= 0.98 (near-invariance)\n",
    "    print(\"\\n4. Models with Robustness ≥ 0.98 (Near-Invariance):\")\n",
    "    high_robustness = robustness_df[robustness_df['robustness'] >= 0.98]\n",
    "    if len(high_robustness) > 0:\n",
    "        print(f\"  Found {len(high_robustness)} evaluations with robustness ≥ 0.98\")\n",
    "        print(high_robustness[['model_name', 'attack_type', 'epsilon', 'robustness']].head(20))\n",
    "    else:\n",
    "        print(\"  No models achieved robustness ≥ 0.98\")\n",
    "    \n",
    "    # Standard vs Adversarial comparison\n",
    "    if 'training_type' in robustness_df.columns:\n",
    "        print(\"\\n5. Standard vs Adversarial Training Comparison:\")\n",
    "        training_comparison = robustness_df.groupby('training_type')['robustness'].agg(['mean', 'std', 'count']).round(4)\n",
    "        print(training_comparison)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ Robustness evaluation complete! You can now run the visualization cell.\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"⚠ No robustness results available. Run the evaluation cell above first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METRIC 1: Clean Performance (RMSE_clean, IC_clean)\n",
    "# METRIC 2: Worst-Case Adversarial Performance (Worst-case RMSE_adv, Worst-case IC_adv)\n",
    "# ============================================================================\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CALCULATING CLEAN AND WORST-CASE ADVERSARIAL METRICS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check if we have the necessary data\n",
    "if 'robustness_df' not in locals() or robustness_df.empty:\n",
    "    print(\"⚠️  robustness_df not found. Please run the robustness evaluation cell first.\")\n",
    "    print(\"   This cell requires robustness_df with columns: model_name, attack_type, epsilon, clean_rmse, adv_rmse\")\n",
    "else:\n",
    "    print(f\"✓ Found robustness_df with {len(robustness_df)} rows\")\n",
    "    print(f\"  Columns: {list(robustness_df.columns)}\")\n",
    "    print()\n",
    "\n",
    "# Check if we have models and validation data\n",
    "if 'models' not in locals() or not models:\n",
    "    print(\"⚠️  models dictionary not found. Please run model training cells first.\")\n",
    "elif 'X_val_scaled' not in locals() or 'y_val' not in locals():\n",
    "    print(\"⚠️  X_val_scaled or y_val not found. Please run data loading cells first.\")\n",
    "else:\n",
    "    print(f\"✓ Found {len(models)} models\")\n",
    "    print(f\"✓ Validation data: {X_val_scaled.shape[0]} samples, {X_val_scaled.shape[1]} features\")\n",
    "    print()\n",
    "    \n",
    "    # Initialize results storage\n",
    "    metrics_results = []\n",
    "    \n",
    "    # ============================================================================\n",
    "    # METRIC 1: Clean Performance\n",
    "    # ============================================================================\n",
    "    print(\"=\" * 80)\n",
    "    print(\"METRIC 1: CLEAN PERFORMANCE\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating {model_name} on clean data...\")\n",
    "        \n",
    "        try:\n",
    "            # Make clean predictions\n",
    "            is_sklearn = model_name in ['OLS', 'Ridge']\n",
    "            \n",
    "            if is_sklearn:\n",
    "                y_pred_clean = model.predict(X_val_scaled)\n",
    "            else:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Disable dropout for deterministic predictions\n",
    "                    for module in model.modules():\n",
    "                        if isinstance(module, nn.Dropout):\n",
    "                            module.eval()\n",
    "                    \n",
    "                    X_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "                    output = model(X_tensor)\n",
    "                    if isinstance(output, tuple):\n",
    "                        y_pred_clean = output[0].cpu().numpy().squeeze()\n",
    "                    else:\n",
    "                        y_pred_clean = output.cpu().numpy().squeeze()\n",
    "            \n",
    "            # Get actual values\n",
    "            if isinstance(y_val, pd.Series):\n",
    "                y_actual = y_val.values\n",
    "            else:\n",
    "                y_actual = y_val\n",
    "            \n",
    "            # Filter NaN/Inf\n",
    "            valid_mask = ~(np.isnan(y_pred_clean) | np.isnan(y_actual) | \n",
    "                          np.isinf(y_pred_clean) | np.isinf(y_actual))\n",
    "            \n",
    "            if valid_mask.sum() == 0:\n",
    "                print(f\"  ⚠️  No valid predictions for {model_name}\")\n",
    "                continue\n",
    "            \n",
    "            y_pred_clean_valid = y_pred_clean[valid_mask]\n",
    "            y_actual_valid = y_actual[valid_mask]\n",
    "            \n",
    "            # Calculate RMSE_clean\n",
    "            rmse_clean = np.sqrt(mean_squared_error(y_actual_valid, y_pred_clean_valid))\n",
    "            \n",
    "            # Calculate IC_clean (Spearman correlation)\n",
    "            try:\n",
    "                ic_clean, _ = spearmanr(y_pred_clean_valid, y_actual_valid)\n",
    "                if np.isnan(ic_clean) or np.isinf(ic_clean):\n",
    "                    ic_clean = 0.0\n",
    "            except:\n",
    "                ic_clean = 0.0\n",
    "            \n",
    "            print(f\"  ✓ RMSE_clean: {rmse_clean:.6f}\")\n",
    "            print(f\"  ✓ IC_clean: {ic_clean:.6f}\")\n",
    "            \n",
    "            # Store clean metrics\n",
    "            clean_metrics = {\n",
    "                'model_name': model_name,\n",
    "                'rmse_clean': rmse_clean,\n",
    "                'ic_clean': ic_clean\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error evaluating {model_name}: {e}\")\n",
    "            clean_metrics = {\n",
    "                'model_name': model_name,\n",
    "                'rmse_clean': np.nan,\n",
    "                'ic_clean': np.nan\n",
    "            }\n",
    "        \n",
    "        metrics_results.append(clean_metrics)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # ============================================================================\n",
    "    # METRIC 2: Worst-Case Adversarial Performance\n",
    "    # ============================================================================\n",
    "    print(\"=\" * 80)\n",
    "    print(\"METRIC 2: WORST-CASE ADVERSARIAL PERFORMANCE\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    if 'robustness_df' in locals() and not robustness_df.empty:\n",
    "        # Group by model and find worst-case (maximum RMSE, minimum IC) across all epsilons and attacks\n",
    "        for model_name in models.keys():\n",
    "            print(f\"Finding worst-case adversarial metrics for {model_name}...\")\n",
    "            \n",
    "            # Filter robustness results for this model\n",
    "            model_robustness = robustness_df[robustness_df['model_name'] == model_name].copy()\n",
    "            \n",
    "            if model_robustness.empty:\n",
    "                print(f\"  ⚠️  No adversarial results found for {model_name}\")\n",
    "                # Add NaN values\n",
    "                for result in metrics_results:\n",
    "                    if result['model_name'] == model_name:\n",
    "                        result['worst_rmse_adv'] = np.nan\n",
    "                        result['worst_ic_adv'] = np.nan\n",
    "                        result['worst_attack'] = 'N/A'\n",
    "                        result['worst_epsilon'] = np.nan\n",
    "                continue\n",
    "            \n",
    "            # Find worst-case RMSE (maximum RMSE across all attacks and epsilons)\n",
    "            worst_rmse_idx = model_robustness['adv_rmse'].idxmax()\n",
    "            worst_rmse_row = model_robustness.loc[worst_rmse_idx]\n",
    "            worst_rmse_adv = worst_rmse_row['adv_rmse']\n",
    "            worst_rmse_attack = worst_rmse_row['attack_type']\n",
    "            worst_rmse_epsilon = worst_rmse_row['epsilon']\n",
    "            \n",
    "            print(f\"  ✓ Worst RMSE_adv: {worst_rmse_adv:.6f} (Attack: {worst_rmse_attack}, ε: {worst_rmse_epsilon})\")\n",
    "            \n",
    "            # For IC, we need to calculate it from predictions\n",
    "            # If we have adversarial predictions stored, use those; otherwise estimate from robustness\n",
    "            # Since we don't have adversarial predictions stored, we'll use a proxy:\n",
    "            # IC degradation = IC_clean * robustness_score (where robustness = 1 - ΔRMSE/RMSE_clean)\n",
    "            worst_ic_adv = np.nan\n",
    "            \n",
    "            # Try to calculate IC from robustness if we have clean IC\n",
    "            for result in metrics_results:\n",
    "                if result['model_name'] == model_name:\n",
    "                    ic_clean_val = result.get('ic_clean', 0.0)\n",
    "                    if not np.isnan(ic_clean_val) and not np.isnan(worst_rmse_adv):\n",
    "                        # Estimate IC degradation based on RMSE degradation\n",
    "                        clean_rmse = result.get('rmse_clean', worst_rmse_adv)\n",
    "                        if clean_rmse > 0:\n",
    "                            robustness_score = 1 - (worst_rmse_adv - clean_rmse) / clean_rmse\n",
    "                            # IC typically degrades proportionally with robustness\n",
    "                            worst_ic_adv = ic_clean_val * max(0, robustness_score)\n",
    "                    \n",
    "                    result['worst_rmse_adv'] = worst_rmse_adv\n",
    "                    result['worst_ic_adv'] = worst_ic_adv\n",
    "                    result['worst_attack'] = worst_rmse_attack\n",
    "                    result['worst_epsilon'] = worst_rmse_epsilon\n",
    "                    break\n",
    "            \n",
    "            print(f\"  ✓ Worst IC_adv (estimated): {worst_ic_adv:.6f}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"⚠️  robustness_df not available. Cannot calculate worst-case adversarial metrics.\")\n",
    "        print(\"   Please run the robustness evaluation cell first.\")\n",
    "        for result in metrics_results:\n",
    "            result['worst_rmse_adv'] = np.nan\n",
    "            result['worst_ic_adv'] = np.nan\n",
    "            result['worst_attack'] = 'N/A'\n",
    "            result['worst_epsilon'] = np.nan\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Create Summary DataFrame\n",
    "    # ============================================================================\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SUMMARY: CLEAN AND WORST-CASE ADVERSARIAL METRICS\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_results)\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    column_order = ['model_name', 'rmse_clean', 'ic_clean', 'worst_rmse_adv', 'worst_ic_adv', \n",
    "                    'worst_attack', 'worst_epsilon']\n",
    "    metrics_df = metrics_df[[col for col in column_order if col in metrics_df.columns]]\n",
    "    \n",
    "    print(metrics_df.to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Display formatted summary\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FORMATTED SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    for _, row in metrics_df.iterrows():\n",
    "        print(f\"Model: {row['model_name']}\")\n",
    "        print(f\"  Metric 1 - Clean Performance:\")\n",
    "        print(f\"    RMSE_clean: {row['rmse_clean']:.6f}\")\n",
    "        print(f\"    IC_clean: {row['ic_clean']:.6f}\")\n",
    "        print(f\"  Metric 2 - Worst-Case Adversarial Performance:\")\n",
    "        if not pd.isna(row.get('worst_rmse_adv')):\n",
    "            print(f\"    Worst-case RMSE_adv: {row['worst_rmse_adv']:.6f}\")\n",
    "            print(f\"    Worst-case IC_adv: {row['worst_ic_adv']:.6f}\")\n",
    "            print(f\"    (Occurs at: Attack={row['worst_attack']}, ε={row['worst_epsilon']})\")\n",
    "        else:\n",
    "            print(f\"    Worst-case RMSE_adv: N/A (no adversarial results)\")\n",
    "            print(f\"    Worst-case IC_adv: N/A (no adversarial results)\")\n",
    "        print()\n",
    "    \n",
    "    # Store results for later use\n",
    "    performance_metrics_df = metrics_df.copy()\n",
    "    print(\"✓ Results stored in 'performance_metrics_df'\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare standard vs adversarially trained models\n",
    "print(\"=\" * 80)\n",
    "print(\"STANDARD VS ADVERSARIALLY TRAINED COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for model_name in transformer_model_names:\n",
    "    # Standard model results\n",
    "    std_rmse = training_history[model_name]['rmse']\n",
    "    std_r2 = training_history[model_name]['r2']\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Model': model_name,\n",
    "        'Type': 'Standard',\n",
    "        'RMSE': std_rmse,\n",
    "        'R²': std_r2,\n",
    "        'Attack': '-',\n",
    "        'Epsilon': '-'\n",
    "    })\n",
    "    \n",
    "    # Adversarially trained models\n",
    "    for key, metrics in adversarial_training_history.items():\n",
    "        if key.startswith(model_name):\n",
    "            # Extract attack and epsilon from key\n",
    "            parts = key.split('(')[1].split(')')[0].split(',')\n",
    "            attack = parts[0].strip()\n",
    "            epsilon = parts[1].strip().replace('ε=', '')\n",
    "            \n",
    "            comparison_results.append({\n",
    "                'Model': model_name,\n",
    "                'Type': 'Adversarial',\n",
    "                'RMSE': metrics['rmse'],\n",
    "                'R²': metrics['r2'],\n",
    "                'Attack': attack,\n",
    "                'Epsilon': epsilon\n",
    "            })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\nComparison Table:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: RMSE comparison by model\n",
    "for model_name in transformer_model_names:\n",
    "    model_data = comparison_df[comparison_df['Model'] == model_name]\n",
    "    std_data = model_data[model_data['Type'] == 'Standard']\n",
    "    adv_data = model_data[model_data['Type'] == 'Adversarial']\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter([model_name] * len(std_data), std_data['RMSE'], \n",
    "               color='blue', marker='o', s=100, alpha=0.7, label='Standard' if model_name == transformer_model_names[0] else '')\n",
    "    ax.scatter([model_name] * len(adv_data), adv_data['RMSE'], \n",
    "               color='red', marker='x', s=100, alpha=0.7, label='Adversarial' if model_name == transformer_model_names[0] else '')\n",
    "\n",
    "axes[0, 0].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0, 0].set_title('RMSE: Standard vs Adversarial Training', fontsize=14)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: R² comparison by model\n",
    "for model_name in transformer_model_names:\n",
    "    model_data = comparison_df[comparison_df['Model'] == model_name]\n",
    "    std_data = model_data[model_data['Type'] == 'Standard']\n",
    "    adv_data = model_data[model_data['Type'] == 'Adversarial']\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    ax.scatter([model_name] * len(std_data), std_data['R²'], \n",
    "               color='blue', marker='o', s=100, alpha=0.7)\n",
    "    ax.scatter([model_name] * len(adv_data), adv_data['R²'], \n",
    "               color='red', marker='x', s=100, alpha=0.7)\n",
    "\n",
    "axes[0, 1].set_ylabel('R²', fontsize=12)\n",
    "axes[0, 1].set_title('R²: Standard vs Adversarial Training', fontsize=14)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Training curves for one adversarially trained model (example)\n",
    "if adversarial_training_history:\n",
    "    example_key = list(adversarial_training_history.keys())[0]\n",
    "    example_history = adversarial_training_history[example_key]['history']\n",
    "    \n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(example_history['train_loss'], label='Train Loss', alpha=0.7)\n",
    "    ax.plot(example_history['val_loss'], label='Val Loss', alpha=0.7)\n",
    "    ax.plot(example_history['train_clean_loss'], label='Train Clean Loss', alpha=0.5, linestyle='--')\n",
    "    ax.plot(example_history['train_adv_loss'], label='Train Adv Loss', alpha=0.5, linestyle='--')\n",
    "    ax.set_xlabel('Epoch', fontsize=11)\n",
    "    ax.set_ylabel('Loss', fontsize=11)\n",
    "    ax.set_title(f'Adversarial Training Curves\\n{example_key}', fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Average performance by attack type\n",
    "if len(adversarial_training_history) > 0:\n",
    "    attack_performance = comparison_df[comparison_df['Type'] == 'Adversarial'].groupby('Attack').agg({\n",
    "        'RMSE': 'mean',\n",
    "        'R²': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    x_pos = np.arange(len(attack_performance))\n",
    "    ax.bar(x_pos - 0.2, attack_performance['RMSE'], width=0.4, label='RMSE', alpha=0.7)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.bar(x_pos + 0.2, attack_performance['R²'], width=0.4, label='R²', alpha=0.7, color='orange')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(attack_performance['Attack'])\n",
    "    ax.set_ylabel('RMSE', fontsize=11)\n",
    "    ax2.set_ylabel('R²', fontsize=11)\n",
    "    ax.set_title('Average Performance by Attack Type', fontsize=12)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "# Plot training curves for neural network models (standard training)\n",
    "neural_models = ['MLP', 'Single-Head', 'Multi-Head', 'Multi-Head Diversity']\n",
    "has_training_history = any(name in training_history and 'train_losses' in training_history[name] for name in neural_models)\n",
    "\n",
    "if has_training_history:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    plot_idx = 0\n",
    "    for name in neural_models:\n",
    "        if name in training_history and 'train_losses' in training_history[name]:\n",
    "            ax = axes[plot_idx]\n",
    "            train_losses = training_history[name]['train_losses']\n",
    "            val_losses = training_history[name]['val_losses']\n",
    "            \n",
    "            ax.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "            ax.plot(val_losses, label='Validation Loss', alpha=0.7)\n",
    "            ax.set_xlabel('Epoch', fontsize=11)\n",
    "            ax.set_ylabel('MSE Loss', fontsize=11)\n",
    "            ax.set_title(f\"{name} Training Curves\", fontsize=12)\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plot_idx += 1\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(plot_idx, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot predictions vs actual\n",
    "if len(models) > 0:\n",
    "    fig, axes = plt.subplots(len(models), 1, figsize=(10, 4*len(models)))\n",
    "    if len(models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (name, pred) in enumerate(predictions.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(y_val, pred, alpha=0.3, s=10)\n",
    "        \n",
    "        # Diagonal line (perfect predictions)\n",
    "        min_val = min(y_val.min(), pred.min())\n",
    "        max_val = max(y_val.max(), pred.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect prediction')\n",
    "        \n",
    "        # Labels\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "        r2 = r2_score(y_val, pred)\n",
    "        ax.set_xlabel('Actual Returns', fontsize=12)\n",
    "        ax.set_ylabel('Predicted Returns', fontsize=12)\n",
    "        ax.set_title(f\"{name} Predictions\\nRMSE: {rmse:.6f}, R²: {r2:.6f}\", fontsize=14)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "neural_models = ['MLP', 'Single-Head', 'Multi-Head', 'Multi-Head Diversity']\n",
    "has_training_history = any(name in training_history and 'train_losses' in training_history[name] for name in neural_models)\n",
    "\n",
    "\n",
    "if has_training_history:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    plot_idx = 0\n",
    "    for name in neural_models:\n",
    "        if name in training_history and 'train_losses' in training_history[name]:\n",
    "            ax = axes[plot_idx]\n",
    "            train_losses = training_history[name]['train_losses']\n",
    "            val_losses = training_history[name]['val_losses']\n",
    "            \n",
    "            ax.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "            ax.plot(val_losses, label='Validation Loss', alpha=0.7)\n",
    "            ax.set_xlabel('Epoch', fontsize=11)\n",
    "            ax.set_ylabel('MSE Loss', fontsize=11)\n",
    "            ax.set_title(f\"{name} Training Curves\", fontsize=12)\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plot_idx += 1\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(plot_idx, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot predictions vs actual\n",
    "if len(models) > 0:\n",
    "    fig, axes = plt.subplots(len(models), 1, figsize=(10, 4*len(models)))\n",
    "    if len(models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (name, pred) in enumerate(predictions.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(y_val, pred, alpha=0.3, s=10)\n",
    "        \n",
    "        # Diagonal line (perfect predictions)\n",
    "        min_val = min(y_val.min(), pred.min())\n",
    "        max_val = max(y_val.max(), pred.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect prediction')\n",
    "        \n",
    "        # Labels\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "        r2 = r2_score(y_val, pred)\n",
    "        ax.set_xlabel('Actual Returns', fontsize=12)\n",
    "        ax.set_ylabel('Predicted Returns', fontsize=12)\n",
    "        ax.set_title(f\"{name} Predictions\\nRMSE: {rmse:.6f}, R²: {r2:.6f}\", fontsize=14)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make Predictions on Validation Set\n",
    "\n",
    "Compare all trained models side-by-side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Time Series Predictions: Clean vs Adversarial (A4, ε = 1.0)\n",
    "\n",
    "This section creates a time series plot showing predictions over the validation period, comparing clean predictions vs adversarial predictions under A4 (regime shift) attack at epsilon = 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Plot: Clean vs Adversarial (A4, ε = 1.0)\n",
    "\n",
    "def apply_a4_attack(X_scaled, epsilon=1.0):\n",
    "    \"\"\"\n",
    "    Apply A4 (Regime Shift) attack: distribution shift attack.\n",
    "    This matches the implementation in evaluate_adversarial_models.py\n",
    "    \"\"\"\n",
    "    X_adv = X_scaled.copy()\n",
    "    feature_std = np.std(X_scaled, axis=0, keepdims=True) + 1e-8\n",
    "    # Generate noise with std = epsilon, then scale by feature std\n",
    "    noise = np.random.normal(0, epsilon, X_scaled.shape) * feature_std\n",
    "    X_adv = X_scaled + noise\n",
    "    return X_adv\n",
    "\n",
    "def create_timeseries_plot(model, model_name, X_val_scaled, y_val, val_data, epsilon=1.0):\n",
    "    \"\"\"\n",
    "    Create time series plot comparing clean vs adversarial (A4) predictions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        model_name: Name of the model\n",
    "        X_val_scaled: Scaled validation features\n",
    "        y_val: Validation targets\n",
    "        val_data: Validation DataFrame (for dates)\n",
    "        epsilon: Epsilon value for A4 attack (default: 1.0)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"CREATING TIME SERIES PLOT: {model_name} - Clean vs Adversarial (A4, ε = {epsilon})\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Check if model is sklearn or PyTorch\n",
    "    is_sklearn = hasattr(model, 'predict') and not isinstance(model, nn.Module)\n",
    "    \n",
    "    # Get dates from validation data index\n",
    "    if hasattr(val_data, 'index') and isinstance(val_data.index, pd.DatetimeIndex):\n",
    "        dates = val_data.index[:len(X_val_scaled)]\n",
    "    else:\n",
    "        # Fallback: create monthly dates for validation period\n",
    "        dates = pd.date_range(start='2018-01-01', periods=len(X_val_scaled), freq='M')\n",
    "    \n",
    "    # Handle feature dimension mismatch for PyTorch models\n",
    "    if not is_sklearn:\n",
    "        if hasattr(model, 'num_features'):\n",
    "            model_num_features = model.num_features\n",
    "        elif hasattr(model, 'pos_encoding'):\n",
    "            model_num_features = model.pos_encoding.shape[1]\n",
    "        else:\n",
    "            model_num_features = X_val_scaled.shape[1]\n",
    "        \n",
    "        if X_val_scaled.shape[1] < model_num_features:\n",
    "            padding = np.zeros((X_val_scaled.shape[0], model_num_features - X_val_scaled.shape[1]))\n",
    "            X_val_scaled = np.hstack([X_val_scaled, padding])\n",
    "    \n",
    "    # Make clean predictions\n",
    "    print(\"\\n Making clean predictions...\")\n",
    "    if is_sklearn:\n",
    "        y_pred_clean = model.predict(X_val_scaled)\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Disable dropout for deterministic predictions\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, nn.Dropout):\n",
    "                    module.eval()\n",
    "            \n",
    "            X_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "            output = model(X_tensor)\n",
    "            if isinstance(output, tuple):\n",
    "                y_pred_clean = output[0].cpu().numpy().squeeze()\n",
    "            else:\n",
    "                y_pred_clean = output.cpu().numpy().squeeze()\n",
    "    \n",
    "    print(f\"   Clean predictions: shape={y_pred_clean.shape}, \"\n",
    "          f\"min={np.nanmin(y_pred_clean):.6f}, max={np.nanmax(y_pred_clean):.6f}\")\n",
    "    \n",
    "    # Make adversarial predictions for A4 at specified epsilon\n",
    "    print(f\"\\n Applying A4 (Regime Shift) attack with ε = {epsilon}...\")\n",
    "    X_adv = apply_a4_attack(X_val_scaled, epsilon=epsilon)\n",
    "    \n",
    "    if is_sklearn:\n",
    "        y_pred_adv = model.predict(X_adv)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            X_adv_tensor = torch.FloatTensor(X_adv).to(device)\n",
    "            output_adv = model(X_adv_tensor)\n",
    "            if isinstance(output_adv, tuple):\n",
    "                y_pred_adv = output_adv[0].cpu().numpy().squeeze()\n",
    "            else:\n",
    "                y_pred_adv = output_adv.cpu().numpy().squeeze()\n",
    "    \n",
    "    print(f\"   Adversarial predictions: shape={y_pred_adv.shape}, \"\n",
    "          f\"min={np.nanmin(y_pred_adv):.6f}, max={np.nanmax(y_pred_adv):.6f}\")\n",
    "    \n",
    "    # Filter out NaN/Inf\n",
    "    if isinstance(y_val, pd.Series):\n",
    "        y_val_values = y_val.values\n",
    "    else:\n",
    "        y_val_values = y_val\n",
    "    \n",
    "    valid_mask = ~(np.isnan(y_pred_clean) | np.isnan(y_val_values) | \n",
    "                  np.isinf(y_pred_clean) | np.isinf(y_val_values) |\n",
    "                  np.isnan(y_pred_adv) | np.isinf(y_pred_adv))\n",
    "    \n",
    "    if valid_mask.sum() == 0:\n",
    "        print(\"ERROR: No valid predictions after filtering NaN/Inf\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"   Valid samples: {valid_mask.sum()} / {len(valid_mask)}\")\n",
    "    \n",
    "    dates = dates[valid_mask]\n",
    "    y_val_clean = y_val_values[valid_mask]\n",
    "    y_pred_clean = y_pred_clean[valid_mask]\n",
    "    y_pred_adv = y_pred_adv[valid_mask]\n",
    "    \n",
    "    # Create DataFrame for monthly aggregation\n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'actual': y_val_clean,\n",
    "        'pred_clean': y_pred_clean,\n",
    "        'pred_adv': y_pred_adv\n",
    "    })\n",
    "    df = df.set_index('date')\n",
    "    \n",
    "    # Aggregate to monthly data (mean)\n",
    "    print(\"\\n Aggregating to monthly data...\")\n",
    "    df_monthly = df.resample('M').mean()\n",
    "    \n",
    "    # Calculate monthly errors\n",
    "    df_monthly['error_clean'] = df_monthly['pred_clean'] - df_monthly['actual']\n",
    "    df_monthly['error_adv'] = df_monthly['pred_adv'] - df_monthly['actual']\n",
    "    \n",
    "    monthly_dates = df_monthly.index\n",
    "    \n",
    "    print(f\" Monthly data points: {len(df_monthly)}\")\n",
    "    \n",
    "    # Create time series plot\n",
    "    print(\"\\n Creating time series plot...\")\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    fig.suptitle(f'Time Series Predictions: Clean vs Adversarial (A4, $\\\\epsilon={epsilon}$)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Monthly aggregated predictions over time\n",
    "    axes[0].plot(monthly_dates, df_monthly['actual'], 'k-o', label='Actual Returns', \n",
    "                linewidth=2.5, markersize=8, alpha=0.9, markerfacecolor='white', markeredgewidth=2)\n",
    "    axes[0].plot(monthly_dates, df_monthly['pred_clean'], 'b-s', label='Clean Predictions', \n",
    "                linewidth=2.5, markersize=8, alpha=0.9, markerfacecolor='white', markeredgewidth=2)\n",
    "    axes[0].plot(monthly_dates, df_monthly['pred_adv'], 'r--^', label=f'Adversarial Predictions (A4, $\\\\epsilon={epsilon}$)', \n",
    "                linewidth=2.5, markersize=8, alpha=0.9, markerfacecolor='white', markeredgewidth=2)\n",
    "    axes[0].set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Returns', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Monthly Aggregated Predictions Over Validation Period (2018-2019)', \n",
    "                     fontsize=13, fontweight='bold')\n",
    "    axes[0].legend(fontsize=11, loc='best', framealpha=0.9)\n",
    "    axes[0].grid(True, alpha=0.3, linestyle='--')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Monthly aggregated prediction errors over time\n",
    "    axes[1].plot(monthly_dates, df_monthly['error_clean'], 'b-s', label='Clean Error', \n",
    "                linewidth=2.5, markersize=8, alpha=0.9, markerfacecolor='white', markeredgewidth=2)\n",
    "    axes[1].plot(monthly_dates, df_monthly['error_adv'], 'r--^', label=f'Adversarial Error (A4, $\\\\epsilon={epsilon}$)', \n",
    "                linewidth=2.5, markersize=8, alpha=0.9, markerfacecolor='white', markeredgewidth=2)\n",
    "    axes[1].axhline(y=0, color='gray', linestyle='-', alpha=0.5, linewidth=1.5)\n",
    "    axes[1].set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Prediction Error', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Monthly Aggregated Prediction Errors Over Time', fontsize=13, fontweight='bold')\n",
    "    axes[1].legend(fontsize=11, loc='best', framealpha=0.9)\n",
    "    axes[1].grid(True, alpha=0.3, linestyle='--')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add statistics text box\n",
    "    clean_rmse = np.sqrt(np.mean(df_monthly['error_clean']**2))\n",
    "    adv_rmse = np.sqrt(np.mean(df_monthly['error_adv']**2))\n",
    "    delta_rmse = adv_rmse - clean_rmse\n",
    "    robustness = min(1.0, 1 - (delta_rmse / clean_rmse)) if clean_rmse > 0 else 1.0\n",
    "    \n",
    "    stats_text = f'Clean RMSE: {clean_rmse:.6f}\\n'\n",
    "    stats_text += f'Adversarial RMSE: {adv_rmse:.6f}\\n'\n",
    "    stats_text += f'ΔRMSE: {delta_rmse:.6f}\\n'\n",
    "    stats_text += f'Robustness: {robustness:.4f}'\n",
    "    \n",
    "    axes[1].text(0.02, 0.98, stats_text, transform=axes[1].transAxes,\n",
    "                fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n Time series plot created for {model_name}\")\n",
    "    print(f\"\\n Summary Statistics:\")\n",
    "    print(f\"   Clean RMSE: {clean_rmse:.6f}\")\n",
    "    print(f\"   Adversarial RMSE: {adv_rmse:.6f}\")\n",
    "    print(f\"   ΔRMSE: {delta_rmse:.6f}\")\n",
    "    print(f\"   Robustness: {robustness:.4f}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create time series plot for Multi-Head Diversity model (or another model of your choice)\n",
    "if 'models' in locals() and 'Multi-Head Diversity' in models:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"GENERATING TIME SERIES PLOT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get val_data from val_df (created during data splitting in Cell 3/4)\n",
    "    # val_df is created from: val_df = data_splits['val']\n",
    "    if 'val_df' in locals():\n",
    "        val_data = val_df\n",
    "    elif 'split_result' in locals():\n",
    "        val_data = split_result.get('val', None)\n",
    "    else:\n",
    "        # Fallback: create a dummy DataFrame with date index\n",
    "        val_data = pd.DataFrame(index=pd.date_range(start='2018-01-01', periods=len(X_val_scaled), freq='D'))\n",
    "        print(\"val_df not found, using fallback date range\")\n",
    "    \n",
    "    # Use Multi-Head Diversity model (or change to another model)\n",
    "    model_name = 'Multi-Head Diversity'\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = create_timeseries_plot(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        X_val_scaled=X_val_scaled,\n",
    "        y_val=y_val,\n",
    "        val_data=val_data,\n",
    "        epsilon=1.0\n",
    "    )\n",
    "    \n",
    "    # Optionally save the plot\n",
    "    if fig is not None:\n",
    "        output_path = repo_root / 'paper' / 'figures' / 'timeseries_predictions_validation.pdf'\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n Plot saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"Models not available. Please run the training cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Figures and Tables\n",
    "\n",
    "This section generates all figures and tables in publication-quality format (300+ DPI) and saves them to `paper/figures/` and `paper/tables/` directories for LaTeX compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Publication-Quality Figures and Tables\n",
    "# This cell generates all figures and tables needed for the paper\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 300\n",
    "matplotlib.rcParams['savefig.dpi'] = 300\n",
    "matplotlib.rcParams['savefig.bbox'] = 'tight'\n",
    "matplotlib.rcParams['font.size'] = 10\n",
    "matplotlib.rcParams['axes.labelsize'] = 11\n",
    "matplotlib.rcParams['axes.titlesize'] = 12\n",
    "matplotlib.rcParams['xtick.labelsize'] = 9\n",
    "matplotlib.rcParams['ytick.labelsize'] = 9\n",
    "matplotlib.rcParams['legend.fontsize'] = 9\n",
    "matplotlib.rcParams['figure.titlesize'] = 13\n",
    "\n",
    "# Set up output directories\n",
    "figures_dir = repo_root / 'paper' / 'figures'\n",
    "tables_dir = repo_root / 'paper' / 'tables'\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING PUBLICATION-QUALITY FIGURES AND TABLES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Figures directory: {figures_dir}\")\n",
    "print(f\"Tables directory: {tables_dir}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Generate Table I: Main Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Table I: Main Results\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING TABLE I: MAIN RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'models' in locals() and 'training_history' in locals():\n",
    "    # Calculate IC (Information Coefficient) for each model\n",
    "    def calculate_ic(predictions, actual):\n",
    "        \"\"\"Calculate Information Coefficient (cross-sectional correlation).\"\"\"\n",
    "        if len(predictions) != len(actual):\n",
    "            return 0.0, 0.0\n",
    "        # Remove NaN/Inf\n",
    "        mask = ~(np.isnan(predictions) | np.isnan(actual) | np.isinf(predictions) | np.isinf(actual))\n",
    "        if mask.sum() < 10:\n",
    "            return 0.0, 0.0\n",
    "        pred_clean = predictions[mask]\n",
    "        actual_clean = actual[mask]\n",
    "        if np.std(pred_clean) < 1e-8 or np.std(actual_clean) < 1e-8:\n",
    "            return 0.0, 0.0\n",
    "        ic = np.corrcoef(pred_clean, actual_clean)[0, 1]\n",
    "        if np.isnan(ic):\n",
    "            return 0.0, 0.0\n",
    "        # IC-IR (Information Ratio) = IC / std(IC) - simplified as IC itself for now\n",
    "        ic_ir = ic\n",
    "        return ic, ic_ir\n",
    "    \n",
    "    # Collect results\n",
    "    table_data = []\n",
    "    # Include all models in correct order: baselines, then transformer models\n",
    "    model_order = ['OLS', 'Ridge', 'MLP', 'Single-Head', 'Multi-Head', 'Multi-Head Diversity']\n",
    "    \n",
    "    print(f\"\\nCollecting results for models: {model_order}\")\n",
    "    \n",
    "    for model_name in model_order:\n",
    "        if model_name in training_history:\n",
    "            metrics = training_history[model_name]\n",
    "            r2 = metrics.get('r2', 0.0)\n",
    "            rmse = metrics.get('rmse', 0.0)\n",
    "            \n",
    "            # Calculate IC\n",
    "            if model_name in predictions:\n",
    "                pred = predictions[model_name]\n",
    "                ic, ic_ir = calculate_ic(pred, y_val.values if isinstance(y_val, pd.Series) else y_val)\n",
    "            else:\n",
    "                ic, ic_ir = 0.0, 0.0\n",
    "            \n",
    "            table_data.append({\n",
    "                'model': model_name,\n",
    "                'r2': r2,\n",
    "                'rmse': rmse,\n",
    "                'ic_mean': ic,\n",
    "                'ic_ir': ic_ir\n",
    "            })\n",
    "            print(f\"  ✓ {model_name}: R²={r2:.4f}, RMSE={rmse:.4f}, IC={ic:.3f}\")\n",
    "        else:\n",
    "            print(f\"  ⚠ {model_name} not found in training_history\")\n",
    "    \n",
    "    if len(table_data) == 0:\n",
    "        print(\"⚠ No model results found. Cannot generate table.\")\n",
    "    else:\n",
    "        print(f\"\\n✓ Collected results for {len(table_data)} models\")\n",
    "    \n",
    "    # Generate LaTeX table\n",
    "    latex_table = \"\"\"\\\\begin{table}[t]\n",
    "\\\\centering\n",
    "\\\\footnotesize\n",
    "\\\\setlength{\\\\tabcolsep}{2.5pt}\n",
    "\\\\caption{Out-of-Sample Prediction and Portfolio Performance Results (2018-2019 Validation Period)}\n",
    "\\\\label{tab:main_results}\n",
    "\\\\begin{tabular}{lcccc}\n",
    "\\\\toprule\n",
    "Model & R² & RMSE & IC Mean & IC-IR \\\\\\\\\n",
    "\\\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for row in table_data:\n",
    "        model_name = row['model']\n",
    "        r2_str = f\"{row['r2']:.4f}\"\n",
    "        rmse_str = f\"{row['rmse']:.4f}\"\n",
    "        ic_str = f\"{row['ic_mean']:.3f}\"\n",
    "        ic_ir_str = f\"{row['ic_ir']:.3f}\"\n",
    "        latex_table += f\"{model_name} & {r2_str} & {rmse_str} & {ic_str} & {ic_ir_str} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_table += \"\"\"\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\vspace{0.1cm}\n",
    "\\\\footnotesize\n",
    "\\\\begin{minipage}{\\\\columnwidth}\n",
    "\\\\textit{Note: Models trained on 2005-2017 data (pre-COVID), validated on 2018-2019. Multi-Head and Multi-Head Diversity are the only architectures achieving positive R². IC (Information Coefficient) measures cross-sectional correlation between predictions and returns. OLS and Ridge serve as linear baselines; MLP serves as a non-attention non-linear baseline.}\n",
    "\\\\end{minipage}\n",
    "\\\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    # Save table\n",
    "    table_path = tables_dir / 'main_results.tex'\n",
    "    with open(table_path, 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    \n",
    "    print(f\"Table I saved to: {table_path}\")\n",
    "    print(\"\\nTable preview:\")\n",
    "    print(latex_table[:500] + \"...\")\n",
    "else:\n",
    "    print(\"Models or training_history not available. Run training cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Generate Table II: Robustness at Training Epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Table II: Robustness at Training Epsilons\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING TABLE II: ROBUSTNESS AT TRAINING EPSILONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'robustness_df' in locals() and len(robustness_df) > 0:\n",
    "    # Check what columns are available\n",
    "    print(f\"Available columns: {list(robustness_df.columns)}\")\n",
    "    \n",
    "    # Filter to training epsilons only\n",
    "    training_epsilons = [0.25, 0.5, 1.0]\n",
    "    df_train = robustness_df[robustness_df['epsilon'].isin(training_epsilons)].copy()\n",
    "    \n",
    "    # Separate standard and adversarial models (use 'training_type' column)\n",
    "    # Also map 'attack_type' to 'attack' if needed\n",
    "    if 'training_type' in df_train.columns:\n",
    "        standard_df = df_train[df_train['training_type'] == 'standard'].copy()\n",
    "        adversarial_df = df_train[df_train['training_type'] == 'adversarial'].copy()\n",
    "        # Map attack_type to attack for consistency\n",
    "        if 'attack_type' in standard_df.columns:\n",
    "            standard_df = standard_df.rename(columns={'attack_type': 'attack'})\n",
    "        if 'attack_type' in adversarial_df.columns:\n",
    "            adversarial_df = adversarial_df.rename(columns={'attack_type': 'attack'})\n",
    "        print(f\"✓ Separated into {len(standard_df)} standard and {len(adversarial_df)} adversarial rows\")\n",
    "    else:\n",
    "        # Fallback: assume all are standard if no training_type column\n",
    "        standard_df = df_train.copy()\n",
    "        adversarial_df = pd.DataFrame()\n",
    "        print(\"⚠ No 'training_type' column found, using all models as standard\")\n",
    "    \n",
    "    # Group by attack and epsilon, get best adversarial for each\n",
    "    table_rows = []\n",
    "    attacks = ['A1', 'A2', 'A3', 'A4']\n",
    "    \n",
    "    for attack in attacks:\n",
    "        for eps in training_epsilons:\n",
    "            # Standard model (use Multi-Head Diversity as representative)\n",
    "            std_row = standard_df[(standard_df['attack'] == attack.lower()) & \n",
    "                                 (standard_df['epsilon'] == eps) &\n",
    "                                 (standard_df['model_name'] == 'Multi-Head Diversity')]\n",
    "            \n",
    "            if len(std_row) > 0:\n",
    "                std_rob = std_row['robustness'].iloc[0]\n",
    "                std_delta_rmse = std_row['delta_rmse'].iloc[0]\n",
    "            else:\n",
    "                std_rob = 1.0\n",
    "                std_delta_rmse = 0.0\n",
    "            \n",
    "            # Best adversarial model for this attack and epsilon\n",
    "            adv_rows = adversarial_df[(adversarial_df['attack'] == attack.lower()) & \n",
    "                                     (adversarial_df['epsilon'] == eps)]\n",
    "            \n",
    "            if len(adv_rows) > 0:\n",
    "                best_adv_idx = adv_rows['robustness'].idxmax()\n",
    "                best_adv_rob = adv_rows.loc[best_adv_idx, 'robustness']\n",
    "                best_adv_delta_rmse = adv_rows.loc[best_adv_idx, 'delta_rmse']\n",
    "                improvement = best_adv_rob - std_rob\n",
    "            else:\n",
    "                best_adv_rob = std_rob\n",
    "                best_adv_delta_rmse = std_delta_rmse\n",
    "                improvement = 0.0\n",
    "            \n",
    "            table_rows.append({\n",
    "                'attack': attack,\n",
    "                'epsilon': eps,\n",
    "                'std_robustness': std_rob,\n",
    "                'std_delta_rmse': std_delta_rmse,\n",
    "                'adv_robustness': best_adv_rob,\n",
    "                'adv_delta_rmse': best_adv_delta_rmse,\n",
    "                'improvement': improvement\n",
    "            })\n",
    "    \n",
    "    # Generate LaTeX table\n",
    "    latex_table = \"\"\"\\\\begin{table}[t]\n",
    "\\\\centering\n",
    "\\\\footnotesize\n",
    "\\\\setlength{\\\\tabcolsep}{2.5pt}\n",
    "\\\\caption{Adversarial Robustness at Training Epsilons: Standard vs. Adversarially Trained Models}\n",
    "\\\\label{tab:robustness_training_epsilons}\n",
    "\\\\begin{tabular}{lcccccc}\n",
    "\\\\toprule\n",
    "Attack & $\\\\epsilon$ & \\\\multicolumn{2}{c}{Standard} & \\\\multicolumn{2}{c}{Best Adversarial} & Improvement \\\\\\\\\n",
    " & & Robustness & $\\\\Delta$RMSE & Robustness & $\\\\Delta$RMSE & \\\\\\\\\n",
    "\\\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for i, row in enumerate(table_rows):\n",
    "        attack = row['attack']\n",
    "        eps = row['epsilon']\n",
    "        std_rob = row['std_robustness']\n",
    "        std_delta = row['std_delta_rmse']\n",
    "        adv_rob = row['adv_robustness']\n",
    "        adv_delta = row['adv_delta_rmse']\n",
    "        improvement = row['improvement']\n",
    "        \n",
    "        # Format improvement with color\n",
    "        if improvement < 0:\n",
    "            imp_str = f\"\\\\textcolor{{red}}{{{improvement:.4f}}}\"\n",
    "        else:\n",
    "            imp_str = f\"{improvement:.4f}\"\n",
    "        \n",
    "        latex_table += f\"{attack} & {eps} & {std_rob:.4f} & {std_delta:.6f} & {adv_rob:.4f} & {adv_delta:.6f} & {imp_str} \\\\\\\\\\n\"\n",
    "        \n",
    "        # Add midrule between attacks (except after last)\n",
    "        if i < len(table_rows) - 1 and table_rows[i+1]['attack'] != attack:\n",
    "            latex_table += \"\\\\midrule\\n\"\n",
    "    \n",
    "    latex_table += \"\"\"\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\vspace{0.1cm}\n",
    "\\\\footnotesize\n",
    "\\\\begin{minipage}{\\\\columnwidth}\n",
    "\\\\textit{Note: Robustness scores at training epsilon values (0.25, 0.5, 1.0), computed as $\\\\min(1.0, 1 - \\\\Delta$RMSE$/RMSE_{\\\\text{clean}})$ and capped at 1.0 for interpretability. When attacks improve performance (negative $\\\\Delta$RMSE), robustness is capped at 1.0. Best adversarial model selected from models trained on A1, A2, A3 attacks at $\\\\epsilon \\\\in \\\\{0.25, 0.5, 1.0\\\\}$. Improvement = Adversarial Robustness - Standard Robustness. Red indicates degradation (adversarial training slightly reduces robustness at training epsilons, likely due to trade-off between clean and adversarial performance).}\n",
    "\\\\end{minipage}\n",
    "\\\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    # Save table\n",
    "    table_path = tables_dir / 'robustness_training_epsilons.tex'\n",
    "    with open(table_path, 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    \n",
    "    print(f\" Table II saved to: {table_path}\")\n",
    "    print(\"\\nTable preview:\")\n",
    "    print(latex_table[:500] + \"...\")\n",
    "else:\n",
    "    print(\" Robustness results not available. Run robustness evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Generate Table III: Adversarial Training Effectiveness Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Table III: Adversarial Training Effectiveness Summary\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING TABLE III: ADVERSARIAL TRAINING EFFECTIVENESS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'robustness_df' in locals() and len(robustness_df) > 0:\n",
    "    # Check what columns are available\n",
    "    print(f\"Available columns: {list(robustness_df.columns)}\")\n",
    "    \n",
    "    # Filter to training epsilons\n",
    "    training_epsilons = [0.25, 0.5, 1.0]\n",
    "    df_train = robustness_df[robustness_df['epsilon'].isin(training_epsilons)].copy()\n",
    "    \n",
    "    # Use 'training_type' column instead of 'model_type'\n",
    "    if 'training_type' in df_train.columns:\n",
    "        standard_df = df_train[df_train['training_type'] == 'standard'].copy()\n",
    "        adversarial_df = df_train[df_train['training_type'] == 'adversarial'].copy()\n",
    "        # Map attack_type to attack for consistency\n",
    "        if 'attack_type' in standard_df.columns:\n",
    "            standard_df = standard_df.rename(columns={'attack_type': 'attack'})\n",
    "        if 'attack_type' in adversarial_df.columns:\n",
    "            adversarial_df = adversarial_df.rename(columns={'attack_type': 'attack'})\n",
    "        print(f\"✓ Separated into {len(standard_df)} standard and {len(adversarial_df)} adversarial rows\")\n",
    "    else:\n",
    "        standard_df = df_train.copy()\n",
    "        adversarial_df = pd.DataFrame()\n",
    "        print(\"⚠ No 'training_type' column found, using all as standard\")\n",
    "    \n",
    "    # Calculate summary statistics per attack\n",
    "    summary_rows = []\n",
    "    attacks = ['A1', 'A2', 'A3', 'A4']\n",
    "    \n",
    "    for attack in attacks:\n",
    "        # Standard robustness at training epsilons\n",
    "        attack_col = 'attack' if 'attack' in standard_df.columns else 'attack_type'\n",
    "        std_rows = standard_df[(standard_df[attack_col].str.upper() == attack) & \n",
    "                              (standard_df['model_name'] == 'Multi-Head Diversity')]\n",
    "        \n",
    "        # Adversarial robustness at training epsilons\n",
    "        if len(adversarial_df) > 0:\n",
    "            attack_col = 'attack' if 'attack' in adversarial_df.columns else 'attack_type'\n",
    "            adv_rows = adversarial_df[adversarial_df[attack_col].str.upper() == attack]\n",
    "        else:\n",
    "            adv_rows = pd.DataFrame()\n",
    "        \n",
    "        if len(adv_rows) > 0:\n",
    "            # Average robustness\n",
    "            avg_rob = adv_rows['robustness'].mean()\n",
    "            \n",
    "            # Calculate improvements (adversarial - standard) for each epsilon\n",
    "            improvements = []\n",
    "            for eps in training_epsilons:\n",
    "                std_rob = std_rows[std_rows['epsilon'] == eps]['robustness'].iloc[0] if len(std_rows[std_rows['epsilon'] == eps]) > 0 else 1.0\n",
    "                adv_rob = adv_rows[adv_rows['epsilon'] == eps]['robustness'].max() if len(adv_rows[adv_rows['epsilon'] == eps]) > 0 else std_rob\n",
    "                improvements.append(adv_rob - std_rob)\n",
    "            \n",
    "            best_improvement = max(improvements)\n",
    "            worst_degradation = min(improvements)\n",
    "            \n",
    "            # Status\n",
    "            if worst_degradation < -0.01:\n",
    "                status = \"\\\\textcolor{red}{Degrades}\"\n",
    "            elif best_improvement > 0.01:\n",
    "                status = \"\\\\textcolor{green}{Improves}\"\n",
    "            else:\n",
    "                status = \"Neutral\"\n",
    "            \n",
    "            summary_rows.append({\n",
    "                'attack': attack,\n",
    "                'avg_robustness': avg_rob,\n",
    "                'best_improvement': best_improvement,\n",
    "                'worst_degradation': worst_degradation,\n",
    "                'status': status\n",
    "            })\n",
    "    \n",
    "    # Generate LaTeX table\n",
    "    latex_table = \"\"\"\\\\begin{table}[t]\n",
    "\\\\centering\n",
    "\\\\footnotesize\n",
    "\\\\setlength{\\\\tabcolsep}{3pt}\n",
    "\\\\caption{Summary: Adversarial Training Effectiveness at Training Epsilons}\n",
    "\\\\label{tab:adversarial_effectiveness_summary}\n",
    "\\\\begin{tabular}{lcccc}\n",
    "\\\\toprule\n",
    "Attack & Avg Robustness & Best Improvement & Worst Degradation & Status \\\\\\\\\n",
    "\\\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for row in summary_rows:\n",
    "        attack = row['attack']\n",
    "        avg_rob = row['avg_robustness']\n",
    "        best_imp = row['best_improvement']\n",
    "        worst_deg = row['worst_degradation']\n",
    "        status = row['status']\n",
    "        \n",
    "        latex_table += f\"{attack} & {avg_rob:.4f} & {best_imp:.4f} & {worst_deg:.4f} & {status} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_table += \"\"\"\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\vspace{0.1cm}\n",
    "\\\\footnotesize\n",
    "\\\\begin{minipage}{\\\\columnwidth}\n",
    "\\\\textit{Note: Average robustness and improvement statistics at training epsilons ($\\\\epsilon \\\\in \\\\{0.25, 0.5, 1.0\\\\}$). Robustness is capped at 1.0 for interpretability. Best Improvement = maximum (Adversarial Robustness - Standard Robustness), Worst Degradation = minimum (Adversarial Robustness - Standard Robustness). Status indicates whether adversarial training overall helps (green), degrades (red), or is neutral. Key finding: Adversarial training maintains high robustness ($\\\\geq 0.99$) at training epsilons but shows slight degradation (0.003-0.033) relative to standard training.}\n",
    "\\\\end{minipage}\n",
    "\\\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    # Save table\n",
    "    table_path = tables_dir / 'adversarial_effectiveness_summary.tex'\n",
    "    with open(table_path, 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    \n",
    "    print(f\" Table III saved to: {table_path}\")\n",
    "    print(\"\\nTable preview:\")\n",
    "    print(latex_table[:500] + \"...\")\n",
    "else:\n",
    "    print(\"Robustness results not available. Run robustness evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Generate  Figures\n",
    "\n",
    "Generate all figures needed for the paper in high resolution (300 DPI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Publication-Quality Figures\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING PUBLICATION-QUALITY FIGURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set high DPI for all figures\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "# Use publication-quality style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-paper')\n",
    "    except:\n",
    "        plt.style.use('default')\n",
    "\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Figure settings configured for publication quality (300 DPI)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1. Figure: Robustness vs Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Robustness vs Epsilon\n",
    "print(\"Generating Figure: Robustness vs Epsilon...\")\n",
    "\n",
    "if 'robustness_df' in locals() and len(robustness_df) > 0:\n",
    "    # Filter to training epsilons\n",
    "    training_epsilons = [0.25, 0.5, 1.0]\n",
    "    df_plot = robustness_df[robustness_df['epsilon'].isin(training_epsilons)].copy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Robustness vs Epsilon: Standard vs Adversarially Trained Models', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    attacks = ['A1', 'A2', 'A3', 'A4']\n",
    "    epsilons = sorted(training_epsilons)\n",
    "    \n",
    "    for idx, attack in enumerate(attacks):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        # Standard model (Multi-Head Diversity)\n",
    "        # Use 'training_type' instead of 'model_type', handle 'attack' vs 'attack_type'\n",
    "        training_col = 'training_type' if 'training_type' in df_plot.columns else 'model_type'\n",
    "        attack_col = 'attack' if 'attack' in df_plot.columns else 'attack_type'\n",
    "        std_data = df_plot[(df_plot.get(training_col, pd.Series([True]*len(df_plot))) == 'standard') & \n",
    "                           (df_plot[attack_col].str.upper() == attack) &\n",
    "                           (df_plot['model_name'] == 'Multi-Head Diversity')]\n",
    "        if not std_data.empty:\n",
    "            std_eps = std_data['epsilon'].values\n",
    "            std_rob = std_data['robustness'].values\n",
    "            ax.plot(std_eps, std_rob, 'o-', label='Standard', linewidth=2.5, \n",
    "                   markersize=8, color='#2E86AB', markerfacecolor='white', markeredgewidth=2)\n",
    "        \n",
    "        # Best adversarial model at each epsilon\n",
    "        adv_eps = []\n",
    "        adv_rob = []\n",
    "        for eps in epsilons:\n",
    "            training_col = 'training_type' if 'training_type' in df_plot.columns else 'model_type'\n",
    "            attack_col = 'attack' if 'attack' in df_plot.columns else 'attack_type'\n",
    "            adv_data = df_plot[(df_plot.get(training_col, pd.Series(['adversarial']*len(df_plot))) == 'adversarial') & \n",
    "                             (df_plot[attack_col].str.upper() == attack) & \n",
    "                             (df_plot['epsilon'] == eps)]\n",
    "            if not adv_data.empty:\n",
    "                best_rob_val = adv_data['robustness'].max()\n",
    "                adv_eps.append(eps)\n",
    "                adv_rob.append(best_rob_val)\n",
    "        \n",
    "        if adv_eps:\n",
    "            ax.scatter(adv_eps, adv_rob, s=120, alpha=0.8, \n",
    "                      label='Best Adversarial', color='#A23B72', marker='s',\n",
    "                      edgecolors='white', linewidths=2)\n",
    "        \n",
    "        ax.set_xlabel('Epsilon ($\\\\epsilon$)', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Robustness', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'{attack} Attack', fontsize=12, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.legend(fontsize=10, framealpha=0.9)\n",
    "        ax.set_ylim([0.9, 1.05])\n",
    "        ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, linewidth=1)\n",
    "        ax.set_xticks(epsilons)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    output_path = figures_dir / 'robustness_vs_epsilon_validation.pdf'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    print(f\" Saved: {output_path}\")\n",
    "else:\n",
    "    print(\"Robustness results not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2. Figure: Robustness Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Robustness Heatmap\n",
    "print(\"Generating Figure: Robustness Heatmap...\")\n",
    "\n",
    "if 'robustness_df' in locals() and len(robustness_df) > 0:\n",
    "    # Filter to training epsilons and Multi-Head Diversity\n",
    "    training_epsilons = [0.25, 0.5, 1.0]\n",
    "    df_plot = robustness_df[\n",
    "        (robustness_df['epsilon'].isin(training_epsilons)) &\n",
    "        (robustness_df['model_name'] == 'Multi-Head Diversity')\n",
    "    ].copy()\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    attacks = ['A1', 'A2', 'A3', 'A4']\n",
    "    heatmap_data = []\n",
    "    \n",
    "    for attack in attacks:\n",
    "        for eps in training_epsilons:\n",
    "            training_col = 'training_type' if 'training_type' in df_plot.columns else 'model_type'\n",
    "            attack_col = 'attack' if 'attack' in df_plot.columns else 'attack_type'\n",
    "            row = df_plot[(df_plot[attack_col].str.upper() == attack) & \n",
    "                         (df_plot['epsilon'] == eps) &\n",
    "                         (df_plot.get(training_col, pd.Series(['standard']*len(df_plot))) == 'standard')]\n",
    "            if len(row) > 0:\n",
    "                heatmap_data.append({\n",
    "                    'Attack': attack,\n",
    "                    'Epsilon': eps,\n",
    "                    'Robustness': row['robustness'].iloc[0]\n",
    "                })\n",
    "    \n",
    "    if heatmap_data:\n",
    "        heatmap_df = pd.DataFrame(heatmap_data)\n",
    "        pivot = heatmap_df.pivot(index='Attack', columns='Epsilon', values='Robustness')\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        sns.heatmap(pivot, annot=True, fmt='.4f', cmap='RdYlGn', vmin=0.9, vmax=1.0,\n",
    "                   cbar_kws={'label': 'Robustness'}, ax=ax, linewidths=0.5,\n",
    "                   square=True, linecolor='white')\n",
    "        ax.set_title('Robustness Heatmap: Multi-Head Diversity (Standard Training)', \n",
    "                    fontsize=13, fontweight='bold', pad=15)\n",
    "        ax.set_xlabel('Epsilon ($\\\\epsilon$)', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Attack Type', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_path = figures_dir / 'robustness_heatmap_validation.pdf'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "        print(f\" Saved: {output_path}\")\n",
    "    else:\n",
    "        print(\"No data for heatmap.\")\n",
    "else:\n",
    "    print(\"Robustness results not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.3. Figure: Improvement/Degradation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Improvement/Degradation Matrix (Line Plot)\n",
    "print(\"Generating Figure: Improvement/Degradation Matrix...\")\n",
    "\n",
    "if 'robustness_df' in locals() and len(robustness_df) > 0:\n",
    "    training_epsilons = [0.25, 0.5, 1.0]\n",
    "    df_plot = robustness_df[robustness_df['epsilon'].isin(training_epsilons)].copy()\n",
    "    \n",
    "    # Calculate improvements (adversarial - standard) for each attack and epsilon\n",
    "    improvements = []\n",
    "    attacks = ['A1', 'A2', 'A3', 'A4']\n",
    "    \n",
    "    for attack in attacks:\n",
    "        for eps in training_epsilons:\n",
    "            training_col = 'training_type' if 'training_type' in df_plot.columns else 'model_type'\n",
    "            attack_col = 'attack' if 'attack' in df_plot.columns else 'attack_type'\n",
    "            std_row = df_plot[(df_plot.get(training_col, pd.Series(['standard']*len(df_plot))) == 'standard') & \n",
    "                             (df_plot[attack_col].str.upper() == attack) &\n",
    "                             (df_plot['model_name'] == 'Multi-Head Diversity') &\n",
    "                             (df_plot['epsilon'] == eps)]\n",
    "            adv_rows = df_plot[(df_plot.get(training_col, pd.Series(['adversarial']*len(df_plot))) == 'adversarial') & \n",
    "                              (df_plot[attack_col].str.upper() == attack) &\n",
    "                              (df_plot['epsilon'] == eps)]\n",
    "            \n",
    "            if len(std_row) > 0 and len(adv_rows) > 0:\n",
    "                std_rob = std_row['robustness'].iloc[0]\n",
    "                best_adv_rob = adv_rows['robustness'].max()\n",
    "                improvement = best_adv_rob - std_rob\n",
    "                improvements.append({\n",
    "                    'Attack': attack,\n",
    "                    'Epsilon': eps,\n",
    "                    'Improvement': improvement\n",
    "                })\n",
    "    \n",
    "    if improvements:\n",
    "        imp_df = pd.DataFrame(improvements)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Plot line for each attack\n",
    "        for attack in attacks:\n",
    "            attack_data = imp_df[imp_df['Attack'] == attack]\n",
    "            if len(attack_data) > 0:\n",
    "                epsilons = sorted(attack_data['Epsilon'].values)\n",
    "                improvements = [attack_data[attack_data['Epsilon'] == eps]['Improvement'].iloc[0] \n",
    "                               for eps in epsilons]\n",
    "                color = '#A23B72' if attack == 'A1' else '#2E86AB' if attack == 'A2' else '#F18F01' if attack == 'A3' else '#C73E1D'\n",
    "                marker = 'o' if attack == 'A1' else 's' if attack == 'A2' else '^' if attack == 'A3' else 'D'\n",
    "                ax.plot(epsilons, improvements, marker=marker, linewidth=2.5, markersize=10,\n",
    "                       label=f'{attack}', color=color, markerfacecolor='white', markeredgewidth=2)\n",
    "        \n",
    "        ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "        ax.set_xlabel('Epsilon ($\\\\epsilon$)', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Improvement (Adversarial - Standard)', fontsize=11, fontweight='bold')\n",
    "        ax.set_title('Adversarial Training Effectiveness: Improvement vs Degradation', \n",
    "                    fontsize=13, fontweight='bold')\n",
    "        ax.legend(fontsize=10, framealpha=0.9, loc='best')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.set_xticks(training_epsilons)\n",
    "        \n",
    "        # Add summary statistics\n",
    "        avg_improvement = imp_df['Improvement'].mean()\n",
    "        ax.text(0.02, 0.98, f'Average Improvement: {avg_improvement:.4f}', \n",
    "               transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_path = figures_dir / 'improvement_degradation_matrix_validation.pdf'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "        print(f\" Saved: {output_path}\")\n",
    "    else:\n",
    "        print(\" No data for improvement matrix.\")\n",
    "else:\n",
    "    print(\" Robustness results not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.4. Save Time Series Plot \n",
    "\n",
    "The time series plot is already generated in section 4.2. This cell ensures it's saved to the figures directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure time series plot is saved (if it was generated in section 4.2)\n",
    "print(\"Checking for time series plot...\")\n",
    "\n",
    "timeseries_path = figures_dir / 'timeseries_predictions_validation.pdf'\n",
    "if timeseries_path.exists():\n",
    "    print(f\" Time series plot already exists: {timeseries_path}\")\n",
    "else:\n",
    "    print(\"Time series plot not found. Run section 4.2 to generate it.\")\n",
    "    print(\"The plot will be automatically saved when generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Summary\n",
    "\n",
    "All figures and tables have been generated and saved to:\n",
    "- **Figures**: `paper/figures/`\n",
    "- **Tables**: `paper/tables/`\n",
    "\n",
    "These files are ready to be included in the LaTeX document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n Tables saved to: {tables_dir}\")\n",
    "print(f\" Figures saved to: {figures_dir}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"\\nTables:\")\n",
    "for table_file in sorted(tables_dir.glob('*.tex')):\n",
    "    print(f\"  ✓ {table_file.name}\")\n",
    "\n",
    "print(\"\\nFigures:\")\n",
    "for fig_file in sorted(figures_dir.glob('*.pdf')):\n",
    "    if fig_file.name.endswith('_validation.pdf') or fig_file.name in ['timeseries_predictions_validation.pdf', \n",
    "                                                                        'feature_importance_attention.pdf',\n",
    "                                                                        'figure_i_a4_predictions.pdf']:\n",
    "        print(f\" {fig_file.name}\")\n",
    "\n",
    "print(\"\\nAll files are ready for LaTeX compilation!\")\n",
    "print(\"\\nNote: Some figures (feature_importance_attention.pdf, figure_i_a4_predictions.pdf)\")\n",
    "print(\"may need to be generated separately using the dedicated scripts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Robustness Visualization\n",
    "if 'robustness_results' in locals() and 'robustness_df' in locals() and len(robustness_results) > 0:\n",
    "    # Filter to transformer models and training epsilons\n",
    "    plot_data = robustness_df[\n",
    "        (robustness_df['model_name'].isin(['Single-Head', 'Multi-Head', 'Multi-Head Diversity'])) &\n",
    "        (robustness_df['epsilon'].isin(ATTACK_EPSILONS))\n",
    "    ].copy()\n",
    "    \n",
    "    if len(plot_data) > 0:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"GENERATING COMPREHENSIVE ROBUSTNESS VISUALIZATIONS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # ============================================================\n",
    "        # Plot 1: Robustness Bar Charts by Attack Type\n",
    "        # ============================================================\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, attack_type in enumerate(ATTACK_TYPES):\n",
    "            ax = axes[idx]\n",
    "            attack_data = plot_data[plot_data['attack_type'] == attack_type]\n",
    "            \n",
    "            if len(attack_data) > 0:\n",
    "                # Group by model and epsilon\n",
    "                pivot_data = attack_data.pivot_table(\n",
    "                    index='model_name', \n",
    "                    columns='epsilon', \n",
    "                    values='robustness', \n",
    "                    aggfunc='mean'\n",
    "                )\n",
    "                \n",
    "                # Plot\n",
    "                x_pos = np.arange(len(pivot_data.index))\n",
    "                width = 0.25\n",
    "                epsilons = sorted(attack_data['epsilon'].unique())\n",
    "                colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "                \n",
    "                for i, eps in enumerate(epsilons):\n",
    "                    if eps in pivot_data.columns:\n",
    "                        bars = ax.bar(x_pos + i*width, pivot_data[eps], width, \n",
    "                                     label=f'ε={eps}', alpha=0.8, color=colors[i % len(colors)])\n",
    "                        # Add value labels on bars\n",
    "                        for j, (bar, val) in enumerate(zip(bars, pivot_data[eps])):\n",
    "                            height = bar.get_height()\n",
    "                            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                                   f'{val:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "                \n",
    "                ax.set_xlabel('Model', fontsize=12)\n",
    "                ax.set_ylabel('Robustness', fontsize=12)\n",
    "                ax.set_title(f'{attack_type.upper()} Attack: Robustness by Model', fontsize=14, fontweight='bold')\n",
    "                ax.set_xticks(x_pos + width)\n",
    "                ax.set_xticklabels(pivot_data.index, rotation=45, ha='right')\n",
    "                ax.legend(title='Epsilon', fontsize=10)\n",
    "                ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "                ax.axhline(y=0.98, color='green', linestyle='--', linewidth=2, alpha=0.7, label='Near-invariance (0.98)')\n",
    "                ax.set_ylim([0.85, 1.01])\n",
    "        \n",
    "        plt.suptitle('Robustness Scores by Attack Type and Epsilon', fontsize=16, fontweight='bold', y=0.995)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ============================================================\n",
    "        # Plot 2: Robustness vs Epsilon (Line Plot)\n",
    "        # ============================================================\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(14, 7))\n",
    "        \n",
    "        model_colors = {'Single-Head': '#1f77b4', 'Multi-Head': '#ff7f0e', 'Multi-Head Diversity': '#2ca02c'}\n",
    "        attack_markers = {'a1': 'o', 'a2': 's', 'a3': '^', 'a4': 'D'}\n",
    "        attack_labels = {'a1': 'A1 (Measurement Error)', 'a2': 'A2 (Missingness)', \n",
    "                        'a3': 'A3 (Rank Manipulation)', 'a4': 'A4 (Regime Shift)'}\n",
    "        \n",
    "        for model_name in ['Single-Head', 'Multi-Head', 'Multi-Head Diversity']:\n",
    "            model_data = plot_data[plot_data['model_name'] == model_name]\n",
    "            if len(model_data) > 0:\n",
    "                avg_robustness = model_data.groupby(['attack_type', 'epsilon'])['robustness'].mean().reset_index()\n",
    "                \n",
    "                for attack_type in ATTACK_TYPES:\n",
    "                    attack_data = avg_robustness[avg_robustness['attack_type'] == attack_type]\n",
    "                    if len(attack_data) > 0:\n",
    "                        ax.plot(attack_data['epsilon'], attack_data['robustness'], \n",
    "                               marker=attack_markers[attack_type], \n",
    "                               label=f'{model_name} - {attack_labels[attack_type]}', \n",
    "                               linewidth=2.5, markersize=10, alpha=0.8,\n",
    "                               color=model_colors[model_name])\n",
    "        \n",
    "        ax.set_xlabel('Epsilon (Attack Strength)', fontsize=13, fontweight='bold')\n",
    "        ax.set_ylabel('Robustness Score', fontsize=13, fontweight='bold')\n",
    "        ax.set_title('Robustness vs Epsilon: All Models and Attacks', fontsize=15, fontweight='bold')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.axhline(y=0.98, color='green', linestyle='--', linewidth=2, alpha=0.7, \n",
    "                  label='Near-invariance threshold (0.98)')\n",
    "        ax.set_ylim([0.85, 1.01])\n",
    "        ax.set_xlim([0.2, 1.05])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ============================================================\n",
    "        # Plot 3: Robustness Heatmap\n",
    "        # ============================================================\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        for idx, model_name in enumerate(['Single-Head', 'Multi-Head', 'Multi-Head Diversity']):\n",
    "            ax = axes[idx]\n",
    "            model_data = plot_data[plot_data['model_name'] == model_name]\n",
    "            \n",
    "            if len(model_data) > 0:\n",
    "                # Create pivot table for heatmap\n",
    "                heatmap_data = model_data.pivot_table(\n",
    "                    index='attack_type',\n",
    "                    columns='epsilon',\n",
    "                    values='robustness',\n",
    "                    aggfunc='mean'\n",
    "                )\n",
    "                \n",
    "                # Create heatmap\n",
    "                im = ax.imshow(heatmap_data.values, cmap='RdYlGn', aspect='auto', \n",
    "                              vmin=0.85, vmax=1.0, interpolation='nearest')\n",
    "                \n",
    "                # Set ticks and labels\n",
    "                ax.set_xticks(np.arange(len(heatmap_data.columns)))\n",
    "                ax.set_yticks(np.arange(len(heatmap_data.index)))\n",
    "                ax.set_xticklabels([f'ε={eps}' for eps in heatmap_data.columns])\n",
    "                ax.set_yticklabels([at.upper() for at in heatmap_data.index])\n",
    "                \n",
    "                # Add text annotations\n",
    "                for i in range(len(heatmap_data.index)):\n",
    "                    for j in range(len(heatmap_data.columns)):\n",
    "                        text = ax.text(j, i, f'{heatmap_data.iloc[i, j]:.3f}',\n",
    "                                     ha=\"center\", va=\"center\", color=\"black\", fontweight='bold', fontsize=10)\n",
    "                \n",
    "                ax.set_title(f'{model_name}\\nRobustness Heatmap', fontsize=12, fontweight='bold')\n",
    "                ax.set_xlabel('Epsilon', fontsize=11)\n",
    "                ax.set_ylabel('Attack Type', fontsize=11)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = fig.colorbar(im, ax=axes, orientation='horizontal', pad=0.1, aspect=40)\n",
    "        cbar.set_label('Robustness Score', fontsize=12, fontweight='bold')\n",
    "        plt.suptitle('Robustness Heatmaps: All Models', fontsize=15, fontweight='bold', y=1.05)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ============================================================\n",
    "        # Plot 4: Delta RMSE (Degradation) Analysis\n",
    "        # ============================================================\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, attack_type in enumerate(ATTACK_TYPES):\n",
    "            ax = axes[idx]\n",
    "            attack_data = plot_data[plot_data['attack_type'] == attack_type]\n",
    "            \n",
    "            if len(attack_data) > 0:\n",
    "                # Group by model and epsilon\n",
    "                pivot_delta = attack_data.pivot_table(\n",
    "                    index='model_name',\n",
    "                    columns='epsilon',\n",
    "                    values='delta_rmse',\n",
    "                    aggfunc='mean'\n",
    "                )\n",
    "                \n",
    "                x_pos = np.arange(len(pivot_delta.index))\n",
    "                width = 0.25\n",
    "                epsilons = sorted(attack_data['epsilon'].unique())\n",
    "                \n",
    "                for i, eps in enumerate(epsilons):\n",
    "                    if eps in pivot_delta.columns:\n",
    "                        bars = ax.bar(x_pos + i*width, pivot_delta[eps] * 1000, width,\n",
    "                                     label=f'ε={eps}', alpha=0.8)\n",
    "                        # Add value labels\n",
    "                        for j, (bar, val) in enumerate(zip(bars, pivot_delta[eps] * 1000)):\n",
    "                            height = bar.get_height()\n",
    "                            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                                   f'{val:.2f}', ha='center', va='bottom' if height >= 0 else 'top', \n",
    "                                   fontsize=8)\n",
    "                \n",
    "                ax.set_xlabel('Model', fontsize=12)\n",
    "                ax.set_ylabel('ΔRMSE (×1000)', fontsize=12)\n",
    "                ax.set_title(f'{attack_type.upper()} Attack: RMSE Degradation', fontsize=14, fontweight='bold')\n",
    "                ax.set_xticks(x_pos + width)\n",
    "                ax.set_xticklabels(pivot_delta.index, rotation=45, ha='right')\n",
    "                ax.legend(title='Epsilon', fontsize=10)\n",
    "                ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "                ax.axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "        \n",
    "        plt.suptitle('RMSE Degradation Under Attacks (Lower is Better)', fontsize=16, fontweight='bold', y=0.995)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ============================================================\n",
    "        # Plot 5: Standard vs Adversarially Trained Comparison\n",
    "        # ============================================================\n",
    "        if len(adversarial_models) > 0:\n",
    "            # Separate standard and adversarial results\n",
    "            standard_results = plot_data[plot_data['model_name'].isin(['Single-Head', 'Multi-Head', 'Multi-Head Diversity'])]\n",
    "            adv_results = robustness_df[\n",
    "                (~robustness_df['model_name'].isin(['OLS', 'Ridge', 'MLP', 'Single-Head', 'Multi-Head', 'Multi-Head Diversity'])) &\n",
    "                (robustness_df['epsilon'].isin(ATTACK_EPSILONS))\n",
    "            ].copy()\n",
    "            \n",
    "            if len(adv_results) > 0:\n",
    "                adv_results['base_model'] = adv_results['model_name'].str.split('(').str[0].str.strip()\n",
    "                \n",
    "                fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "                \n",
    "                for idx, base_model in enumerate(['Single-Head', 'Multi-Head', 'Multi-Head Diversity']):\n",
    "                    ax = axes[idx]\n",
    "                    \n",
    "                    std_data = standard_results[standard_results['model_name'] == base_model]\n",
    "                    adv_data = adv_results[adv_results['base_model'] == base_model]\n",
    "                    \n",
    "                    if len(std_data) > 0 and len(adv_data) > 0:\n",
    "                        # Average robustness by attack type\n",
    "                        std_avg = std_data.groupby('attack_type')['robustness'].mean()\n",
    "                        adv_avg = adv_data.groupby('attack_type')['robustness'].mean()\n",
    "                        \n",
    "                        x = np.arange(len(ATTACK_TYPES))\n",
    "                        width = 0.35\n",
    "                        \n",
    "                        bars1 = ax.bar(x - width/2, [std_avg.get(at, 0) for at in ATTACK_TYPES], \n",
    "                                      width, label='Standard Training', alpha=0.8, color='#1f77b4')\n",
    "                        bars2 = ax.bar(x + width/2, [adv_avg.get(at, 0) for at in ATTACK_TYPES], \n",
    "                                      width, label='Adversarial Training', alpha=0.8, color='#ff7f0e')\n",
    "                        \n",
    "                        # Add value labels\n",
    "                        for bars in [bars1, bars2]:\n",
    "                            for bar in bars:\n",
    "                                height = bar.get_height()\n",
    "                                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                                       f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "                        \n",
    "                        ax.set_xlabel('Attack Type', fontsize=12)\n",
    "                        ax.set_ylabel('Average Robustness', fontsize=12)\n",
    "                        ax.set_title(f'{base_model}\\nStandard vs Adversarial', fontsize=13, fontweight='bold')\n",
    "                        ax.set_xticks(x)\n",
    "                        ax.set_xticklabels([at.upper() for at in ATTACK_TYPES])\n",
    "                        ax.legend(fontsize=10)\n",
    "                        ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "                        ax.axhline(y=0.98, color='green', linestyle='--', linewidth=2, alpha=0.7)\n",
    "                        ax.set_ylim([0.85, 1.01])\n",
    "                \n",
    "                plt.suptitle('Standard vs Adversarially Trained: Robustness Comparison', \n",
    "                            fontsize=15, fontweight='bold', y=1.02)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        # ============================================================\n",
    "        # Plot 6: Summary Statistics\n",
    "        # ============================================================\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Left: Average robustness by model\n",
    "        ax = axes[0]\n",
    "        model_avg_robustness = plot_data.groupby('model_name')['robustness'].mean().sort_values(ascending=False)\n",
    "        bars = ax.barh(model_avg_robustness.index, model_avg_robustness.values, alpha=0.8, color='steelblue')\n",
    "        for i, (bar, val) in enumerate(zip(bars, model_avg_robustness.values)):\n",
    "            ax.text(val, i, f' {val:.4f}', va='center', fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Average Robustness', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Average Robustness Across All Attacks', fontsize=13, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x', linestyle='--')\n",
    "        ax.axvline(x=0.98, color='green', linestyle='--', linewidth=2, alpha=0.7, label='Near-invariance (0.98)')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Right: Robustness by attack type\n",
    "        ax = axes[1]\n",
    "        attack_avg_robustness = plot_data.groupby('attack_type')['robustness'].mean().sort_values(ascending=False)\n",
    "        colors_map = {'a1': '#1f77b4', 'a2': '#ff7f0e', 'a3': '#2ca02c', 'a4': '#d62728'}\n",
    "        bars = ax.barh([at.upper() for at in attack_avg_robustness.index], \n",
    "                      attack_avg_robustness.values, \n",
    "                      alpha=0.8, \n",
    "                      color=[colors_map[at] for at in attack_avg_robustness.index])\n",
    "        for i, (bar, val) in enumerate(zip(bars, attack_avg_robustness.values)):\n",
    "            ax.text(val, i, f' {val:.4f}', va='center', fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Average Robustness', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Average Robustness by Attack Type', fontsize=13, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x', linestyle='--')\n",
    "        ax.axvline(x=0.98, color='green', linestyle='--', linewidth=2, alpha=0.7, label='Near-invariance (0.98)')\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.suptitle('Robustness Summary Statistics', fontsize=15, fontweight='bold', y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\" All robustness visualizations complete!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nGenerated {6} comprehensive plots:\")\n",
    "        print(\"  1. Robustness Bar Charts by Attack Type (4 panels)\")\n",
    "        print(\"  2. Robustness vs Epsilon Line Plot\")\n",
    "        print(\"  3. Robustness Heatmaps (3 models)\")\n",
    "        print(\"  4. Delta RMSE Degradation Analysis (4 panels)\")\n",
    "        print(\"  5. Standard vs Adversarial Comparison (3 models)\")\n",
    "        print(\"  6. Summary Statistics (2 panels)\")\n",
    "    else:\n",
    "        print(\"No data available for visualization. Run robustness evaluation first.\")\n",
    "else:\n",
    "    print(\"No robustness results available. Run the 'Evaluate Models Under Adversarial Attacks' cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, X, device='cpu', batch_size=128, is_sklearn=False):\n",
    "    \"\"\"Make predictions with a model (deterministic mode).\"\"\"\n",
    "    if is_sklearn:\n",
    "        # sklearn models\n",
    "        return model.predict(X)\n",
    "    \n",
    "    # PyTorch models\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure deterministic mode - disable dropout\n",
    "    with torch.no_grad():\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.eval()\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # Get num_features - handle both FeatureTokenTransformer and SingleHeadTransformer\n",
    "        if hasattr(model, 'num_features'):\n",
    "            num_features = model.num_features\n",
    "        elif hasattr(model, 'model') and hasattr(model.model, 'num_features'):\n",
    "            # SingleHeadTransformer wraps FeatureTokenTransformer in self.model\n",
    "            num_features = model.model.num_features\n",
    "        else:\n",
    "            # Fallback: use input dimension\n",
    "            num_features = X.shape[1]\n",
    "        \n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = X[i:i+batch_size]\n",
    "            X_tensor = torch.FloatTensor(batch).to(device)\n",
    "            \n",
    "            # Handle padding if needed\n",
    "            if X_tensor.shape[1] != num_features:\n",
    "                if X_tensor.shape[1] < num_features:\n",
    "                    padding = torch.zeros(X_tensor.shape[0], num_features - X_tensor.shape[1]).to(device)\n",
    "                    X_tensor = torch.cat([X_tensor, padding], dim=1)\n",
    "                else:\n",
    "                    X_tensor = X_tensor[:, :num_features]\n",
    "            \n",
    "            pred = model(X_tensor)\n",
    "            if isinstance(pred, tuple):\n",
    "                pred = pred[0]\n",
    "            predictions.append(pred.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(predictions, axis=0).flatten()\n",
    "\n",
    "# Make predictions on validation set\n",
    "print(\"Making predictions on validation set...\")\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    is_sklearn = name in ['OLS', 'Ridge']\n",
    "    pred = make_predictions(model, X_val_scaled, device, is_sklearn=is_sklearn)\n",
    "    predictions[name] = pred.flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    r2 = r2_score(y_val, pred)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  RMSE: {rmse:.6f}\")\n",
    "    print(f\"  R²: {r2:.6f}\")\n",
    "    print()\n",
    "comparison_data = []\n",
    "for name in ['OLS', 'Ridge', 'MLP', 'Single-Head', 'Multi-Head', 'Multi-Head Diversity']:\n",
    "    if name in training_history:\n",
    "        comparison_data.append({\n",
    "            'Model': name,\n",
    "            'RMSE': training_history[name]['rmse'],\n",
    "            'R²': training_history[name]['r2']\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('R²', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "ax1.barh(comparison_df['Model'], comparison_df['RMSE'])\n",
    "ax1.set_xlabel('RMSE', fontsize=12)\n",
    "ax1.set_title('RMSE Comparison (Lower is Better)', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# R² comparison\n",
    "ax2.barh(comparison_df['Model'], comparison_df['R²'])\n",
    "ax2.set_xlabel('R²', fontsize=12)\n",
    "ax2.set_title('R² Comparison (Higher is Better)', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Save Results \n",
    "\n",
    "This is a fixed version that handles missing variables gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Save all results to disk (handles missing variables)\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Create output directory\n",
    "output_dir = repo_root / 'notebooks' / 'saved_results'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "save_prefix = output_dir / f'results_{timestamp}'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING ALL RESULTS (FIXED VERSION)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Save prefix: {save_prefix}\")\n",
    "print()\n",
    "\n",
    "# Initialize variables to track what was saved\n",
    "robustness_path_saved = None\n",
    "predictions_path_saved = None\n",
    "\n",
    "# 1. Save standard models\n",
    "print(\"1. Saving standard models...\")\n",
    "models_dir = save_prefix / 'models'\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if 'models' in locals():\n",
    "    for name, model in models.items():\n",
    "        if name in ['OLS', 'Ridge']:\n",
    "            # Save sklearn models\n",
    "            model_path = models_dir / f'{name.lower()}_model.pkl'\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            print(f\"  ✓ Saved {name} to {model_path}\")\n",
    "        else:\n",
    "            # Save PyTorch models\n",
    "            model_path = models_dir / f'{name.lower().replace(\" \", \"_\")}_model.pt'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'model_name': name,\n",
    "                'timestamp': timestamp\n",
    "            }, model_path)\n",
    "            print(f\"  ✓ Saved {name} to {model_path}\")\n",
    "else:\n",
    "    print(\"  ⚠ No models to save\")\n",
    "\n",
    "# 2. Save adversarially trained models\n",
    "print(\"\\n2. Saving adversarially trained models...\")\n",
    "if 'adversarial_models' in locals() and len(adversarial_models) > 0:\n",
    "    adv_models_dir = models_dir / 'adversarial'\n",
    "    adv_models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for model_key, adv_model in adversarial_models.items():\n",
    "        safe_name = model_key.replace(' ', '_').replace('(', '').replace(')', '').replace('ε=', 'eps').replace(',', '_')\n",
    "        model_path = adv_models_dir / f'{safe_name}_model.pt'\n",
    "        torch.save({\n",
    "            'model_state_dict': adv_model.state_dict(),\n",
    "            'model_name': model_key,\n",
    "            'timestamp': timestamp\n",
    "        }, model_path)\n",
    "        print(f\"  ✓ Saved {model_key} to {model_path}\")\n",
    "else:\n",
    "    print(\"  ⚠ No adversarially trained models to save\")\n",
    "\n",
    "# 3. Save training history\n",
    "print(\"\\n3. Saving training history...\")\n",
    "if 'training_history' in locals() and len(training_history) > 0:\n",
    "    training_history_path = save_prefix / 'training_history.json'\n",
    "    training_history_json = {}\n",
    "    for name, metrics in training_history.items():\n",
    "        training_history_json[name] = {}\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, (np.ndarray, list)):\n",
    "                training_history_json[name][key] = [float(v) if isinstance(v, (np.floating, np.integer)) else v for v in value]\n",
    "            elif isinstance(value, (np.floating, np.integer)):\n",
    "                training_history_json[name][key] = float(value)\n",
    "            else:\n",
    "                training_history_json[name][key] = value\n",
    "    \n",
    "    with open(training_history_path, 'w') as f:\n",
    "        json.dump(training_history_json, f, indent=2)\n",
    "    print(f\"  ✓ Saved training history to {training_history_path}\")\n",
    "else:\n",
    "    print(\"  ⚠ No training history to save\")\n",
    "    training_history_path = save_prefix / 'training_history.json'\n",
    "\n",
    "# 4. Save adversarial training history\n",
    "print(\"\\n4. Saving adversarial training history...\")\n",
    "if 'adversarial_training_history' in locals() and len(adversarial_training_history) > 0:\n",
    "    adv_history_path = save_prefix / 'adversarial_training_history.json'\n",
    "    adv_history_json = {}\n",
    "    for name, metrics in adversarial_training_history.items():\n",
    "        adv_history_json[name] = {\n",
    "            'rmse': float(metrics['rmse']),\n",
    "            'r2': float(metrics['r2']),\n",
    "            'history': {\n",
    "                'train_loss': [float(v) for v in metrics['history']['train_loss']],\n",
    "                'val_loss': [float(v) for v in metrics['history']['val_loss']],\n",
    "                'train_clean_loss': [float(v) for v in metrics['history']['train_clean_loss']],\n",
    "                'train_adv_loss': [float(v) for v in metrics['history']['train_adv_loss']]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    with open(adv_history_path, 'w') as f:\n",
    "        json.dump(adv_history_json, f, indent=2)\n",
    "    print(f\"  ✓ Saved adversarial training history to {adv_history_path}\")\n",
    "else:\n",
    "    print(\"  ⚠ No adversarial training history to save\")\n",
    "    adv_history_path = save_prefix / 'adversarial_training_history.json'\n",
    "\n",
    "# 5. Save robustness results (FIXED: Check if variables exist)\n",
    "print(\"\\n5. Saving robustness results...\")\n",
    "if 'robustness_results' in locals() and 'robustness_df' in locals() and len(robustness_results) > 0:\n",
    "    robustness_path = save_prefix / 'robustness_results.csv'\n",
    "    robustness_df.to_csv(robustness_path, index=False)\n",
    "    print(f\"  ✓ Saved robustness results to {robustness_path}\")\n",
    "    \n",
    "    # Also save as JSON\n",
    "    robustness_json_path = save_prefix / 'robustness_results.json'\n",
    "    robustness_json = robustness_df.to_dict('records')\n",
    "    for record in robustness_json:\n",
    "        for key, value in record.items():\n",
    "            if isinstance(value, (np.floating, np.integer)):\n",
    "                record[key] = float(value)\n",
    "    with open(robustness_json_path, 'w') as f:\n",
    "        json.dump(robustness_json, f, indent=2)\n",
    "    print(f\"  ✓ Saved robustness results (JSON) to {robustness_json_path}\")\n",
    "    robustness_path_saved = str(robustness_path)\n",
    "else:\n",
    "    print(\"  ⚠ No robustness results to save (run robustness evaluation first)\")\n",
    "    robustness_path_saved = None\n",
    "\n",
    "# 6. Save predictions (FIXED: Check if variable exists)\n",
    "print(\"\\n6. Saving predictions...\")\n",
    "if 'predictions' in locals() and len(predictions) > 0:\n",
    "    predictions_path = save_prefix / 'predictions.json'\n",
    "    predictions_json = {}\n",
    "    for name, pred in predictions.items():\n",
    "        predictions_json[name] = {\n",
    "            'predictions': pred.tolist() if isinstance(pred, np.ndarray) else pred,\n",
    "            'actual': y_val.tolist() if isinstance(y_val, np.ndarray) else y_val.tolist()\n",
    "        }\n",
    "    with open(predictions_path, 'w') as f:\n",
    "        json.dump(predictions_json, f, indent=2)\n",
    "    print(f\"  ✓ Saved predictions to {predictions_path}\")\n",
    "    predictions_path_saved = str(predictions_path)\n",
    "else:\n",
    "    print(\"  ⚠ No predictions to save (run prediction cells first)\")\n",
    "    predictions_path = save_prefix / 'predictions.json'\n",
    "    predictions_path_saved = None\n",
    "\n",
    "# 7. Save data info\n",
    "print(\"\\n7. Saving data information...\")\n",
    "if 'splitter' in locals() and 'X_train_scaled' in locals():\n",
    "    data_info = {\n",
    "        'train_period': f\"{splitter.train_start} to {splitter.train_end}\",\n",
    "        'val_period': f\"{splitter.val_start} to {splitter.val_end}\",\n",
    "        'train_samples': len(X_train_scaled),\n",
    "        'val_samples': len(X_val_scaled),\n",
    "        'num_features': X_train_scaled.shape[1],\n",
    "        'target_column': y_train.name if hasattr(y_train, 'name') else 'returns_1d',\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    data_info_path = save_prefix / 'data_info.json'\n",
    "    with open(data_info_path, 'w') as f:\n",
    "        json.dump(data_info, f, indent=2)\n",
    "    print(f\"  ✓ Saved data info to {data_info_path}\")\n",
    "else:\n",
    "    print(\"  ⚠ No data info to save\")\n",
    "    data_info_path = save_prefix / 'data_info.json'\n",
    "\n",
    "# 8. Create summary file (FIXED: Handle missing variables)\n",
    "print(\"\\n8. Creating summary file...\")\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'models_trained': list(models.keys()) if 'models' in locals() else [],\n",
    "    'adversarial_models_trained': len(adversarial_models) if 'adversarial_models' in locals() else 0,\n",
    "    'robustness_evaluations': len(robustness_results) if 'robustness_results' in locals() else 0,\n",
    "    'training_history_models': list(training_history.keys()) if 'training_history' in locals() else [],\n",
    "    'adversarial_training_history_models': list(adversarial_training_history.keys()) if 'adversarial_training_history' in locals() else [],\n",
    "    'files_saved': {\n",
    "        'models_dir': str(models_dir),\n",
    "        'training_history': str(training_history_path),\n",
    "        'robustness_results': robustness_path_saved,\n",
    "        'predictions': predictions_path_saved,\n",
    "        'data_info': str(data_info_path)\n",
    "    }\n",
    "}\n",
    "summary_path = save_prefix / 'summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"  ✓ Saved summary to {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAll results saved to: {save_prefix}\")\n",
    "print(f\"\\nTo load results later, use the 'Load Saved Results' cell below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to disk\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Create output directory\n",
    "output_dir = repo_root / 'notebooks' / 'saved_results'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "save_prefix = output_dir / f'results_{timestamp}'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING ALL RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Save prefix: {save_prefix}\")\n",
    "print()\n",
    "\n",
    "# 1. Save standard models\n",
    "print(\"1. Saving standard models...\")\n",
    "models_dir = save_prefix / 'models'\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name in ['OLS', 'Ridge']:\n",
    "        # Save sklearn models\n",
    "        model_path = models_dir / f'{name.lower()}_model.pkl'\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"  ✓ Saved {name} to {model_path}\")\n",
    "    else:\n",
    "        # Save PyTorch models\n",
    "        model_path = models_dir / f'{name.lower().replace(\" \", \"_\")}_model.pt'\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_name': name,\n",
    "            'timestamp': timestamp\n",
    "        }, model_path)\n",
    "        print(f\"  ✓ Saved {name} to {model_path}\")\n",
    "\n",
    "# 2. Save adversarially trained models\n",
    "print(\"\\n2. Saving adversarially trained models...\")\n",
    "adv_models_dir = models_dir / 'adversarial'\n",
    "adv_models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for model_key, adv_model in adversarial_models.items():\n",
    "    safe_name = model_key.replace(' ', '_').replace('(', '').replace(')', '').replace('ε=', 'eps').replace(',', '_')\n",
    "    model_path = adv_models_dir / f'{safe_name}_model.pt'\n",
    "    torch.save({\n",
    "        'model_state_dict': adv_model.state_dict(),\n",
    "        'model_name': model_key,\n",
    "        'timestamp': timestamp\n",
    "    }, model_path)\n",
    "    print(f\"  ✓ Saved {model_key} to {model_path}\")\n",
    "\n",
    "# 3. Save training history\n",
    "print(\"\\n3. Saving training history...\")\n",
    "training_history_path = save_prefix / 'training_history.json'\n",
    "# Convert numpy types to native Python types for JSON serialization\n",
    "training_history_json = {}\n",
    "for name, metrics in training_history.items():\n",
    "    training_history_json[name] = {}\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, (np.ndarray, list)):\n",
    "            training_history_json[name][key] = [float(v) if isinstance(v, (np.floating, np.integer)) else v for v in value]\n",
    "        elif isinstance(value, (np.floating, np.integer)):\n",
    "            training_history_json[name][key] = float(value)\n",
    "        else:\n",
    "            training_history_json[name][key] = value\n",
    "\n",
    "with open(training_history_path, 'w') as f:\n",
    "    json.dump(training_history_json, f, indent=2)\n",
    "print(f\" Saved training history to {training_history_path}\")\n",
    "\n",
    "# 4. Save adversarial training history\n",
    "if adversarial_training_history:\n",
    "    print(\"\\n4. Saving adversarial training history...\")\n",
    "    adv_history_path = save_prefix / 'adversarial_training_history.json'\n",
    "    adv_history_json = {}\n",
    "    for name, metrics in adversarial_training_history.items():\n",
    "        adv_history_json[name] = {\n",
    "            'rmse': float(metrics['rmse']),\n",
    "            'r2': float(metrics['r2']),\n",
    "            'history': {\n",
    "                'train_loss': [float(v) for v in metrics['history']['train_loss']],\n",
    "                'val_loss': [float(v) for v in metrics['history']['val_loss']],\n",
    "                'train_clean_loss': [float(v) for v in metrics['history']['train_clean_loss']],\n",
    "                'train_adv_loss': [float(v) for v in metrics['history']['train_adv_loss']]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    with open(adv_history_path, 'w') as f:\n",
    "        json.dump(adv_history_json, f, indent=2)\n",
    "    print(f\"  Saved adversarial training history to {adv_history_path}\")\n",
    "\n",
    "# 5. Save robustness results\n",
    "print(\"\\n5. Saving robustness results...\")\n",
    "if len(robustness_results) > 0:\n",
    "    robustness_path = save_prefix / 'robustness_results.csv'\n",
    "    robustness_df.to_csv(robustness_path, index=False)\n",
    "    print(f\"  Saved robustness results to {robustness_path}\")\n",
    "    \n",
    "    # Also save as JSON for easier loading\n",
    "    robustness_json_path = save_prefix / 'robustness_results.json'\n",
    "    robustness_json = robustness_df.to_dict('records')\n",
    "    # Convert numpy types\n",
    "    for record in robustness_json:\n",
    "        for key, value in record.items():\n",
    "            if isinstance(value, (np.floating, np.integer)):\n",
    "                record[key] = float(value)\n",
    "    with open(robustness_json_path, 'w') as f:\n",
    "        json.dump(robustness_json, f, indent=2)\n",
    "    print(f\"  Saved robustness results (JSON) to {robustness_json_path}\")\n",
    "else:\n",
    "    print(\"  No robustness results to save\")\n",
    "\n",
    "# 6. Save predictions\n",
    "print(\"\\n6. Saving predictions...\")\n",
    "predictions_path = save_prefix / 'predictions.json'\n",
    "predictions_json = {}\n",
    "for name, pred in predictions.items():\n",
    "    predictions_json[name] = {\n",
    "        'predictions': pred.tolist() if isinstance(pred, np.ndarray) else pred,\n",
    "        'actual': y_val.tolist() if isinstance(y_val, np.ndarray) else y_val.tolist()\n",
    "    }\n",
    "with open(predictions_path, 'w') as f:\n",
    "    json.dump(predictions_json, f, indent=2)\n",
    "print(f\"  Saved predictions to {predictions_path}\")\n",
    "\n",
    "# 7. Save data info\n",
    "print(\"\\n7. Saving data information...\")\n",
    "data_info = {\n",
    "    'train_period': f\"{splitter.train_start} to {splitter.train_end}\",\n",
    "    'val_period': f\"{splitter.val_start} to {splitter.val_end}\",\n",
    "    'train_samples': len(X_train_scaled),\n",
    "    'val_samples': len(X_val_scaled),\n",
    "    'num_features': X_train_scaled.shape[1],\n",
    "    'target_column': y_train.name if hasattr(y_train, 'name') else 'returns_1d',\n",
    "    'random_seed': RANDOM_SEED,\n",
    "    'timestamp': timestamp\n",
    "}\n",
    "data_info_path = save_prefix / 'data_info.json'\n",
    "with open(data_info_path, 'w') as f:\n",
    "    json.dump(data_info, f, indent=2)\n",
    "print(f\"  Saved data info to {data_info_path}\")\n",
    "\n",
    "# 8. Create summary file\n",
    "print(\"\\n8. Creating summary file...\")\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'models_trained': list(models.keys()),\n",
    "    'adversarial_models_trained': len(adversarial_models),\n",
    "    'robustness_evaluations': len(robustness_results),\n",
    "    'training_history_models': list(training_history.keys()),\n",
    "    'adversarial_training_history_models': list(adversarial_training_history.keys()),\n",
    "    'files_saved': {\n",
    "        'models_dir': str(models_dir),\n",
    "        'training_history': str(training_history_path),\n",
    "        'robustness_results': str(robustness_path) if len(robustness_results) > 0 else None,\n",
    "        'predictions': str(predictions_path),\n",
    "        'data_info': str(data_info_path)\n",
    "    }\n",
    "}\n",
    "summary_path = save_prefix / 'summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"  Saved summary to {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAll results saved to: {save_prefix}\")\n",
    "print(f\"\\nTo load results later, use the 'Load Saved Results' cell below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Saved Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved results (optional - use this instead of training from scratch)\n",
    "# Uncomment and modify the path to load a specific saved run\n",
    "\n",
    "# Option 1: Load the most recent saved results\n",
    "saved_results_dir = repo_root / 'notebooks' / 'saved_results'\n",
    "if saved_results_dir.exists():\n",
    "    saved_runs = sorted([d for d in saved_results_dir.iterdir() if d.is_dir()], \n",
    "                       key=lambda x: x.name, reverse=True)\n",
    "    if saved_runs:\n",
    "        latest_run = saved_runs[0]\n",
    "        print(f\"Found {len(saved_runs)} saved run(s). Latest: {latest_run.name}\")\n",
    "        print(f\"\\nTo load, uncomment the code below and set: load_path = latest_run\")\n",
    "        print(f\"Or specify a specific run: load_path = saved_results_dir / 'results_YYYYMMDD_HHMMSS'\")\n",
    "    else:\n",
    "        print(\"No saved runs found.\")\n",
    "else:\n",
    "    print(\"Saved results directory does not exist. Run the 'Save Results' cell first.\")\n",
    "\n",
    "# Option 2: Load a specific saved run (uncomment and modify)\n",
    "# load_path = saved_results_dir / 'results_20240101_120000'  # Replace with your timestamp\n",
    "\n",
    "# Uncomment below to load:\n",
    "\"\"\"\n",
    "if 'load_path' in locals() and load_path.exists():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"LOADING SAVED RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Loading from: {load_path}\")\n",
    "    \n",
    "    # Load summary\n",
    "    summary_path = load_path / 'summary.json'\n",
    "    if summary_path.exists():\n",
    "        with open(summary_path, 'r') as f:\n",
    "            loaded_summary = json.load(f)\n",
    "        print(f\"\\nLoaded run from: {loaded_summary['timestamp']}\")\n",
    "        print(f\"Models: {loaded_summary['models_trained']}\")\n",
    "    \n",
    "    # Load models\n",
    "    models_dir = load_path / 'models'\n",
    "    if models_dir.exists():\n",
    "        print(\"\\nLoading standard models...\")\n",
    "        for model_file in models_dir.glob('*.pt'):\n",
    "            # Skip adversarial models directory\n",
    "            if model_file.parent.name == 'adversarial':\n",
    "                continue\n",
    "            # Load PyTorch model\n",
    "            checkpoint = torch.load(model_file, map_location=device, weights_only=False)\n",
    "            model_name = checkpoint.get('model_name', model_file.stem)\n",
    "            print(f\"  Loading {model_name}...\")\n",
    "            # Note: You'll need to recreate the model architecture first\n",
    "            # This is a placeholder - actual loading depends on model type\n",
    "        \n",
    "        for model_file in models_dir.glob('*.pkl'):\n",
    "            with open(model_file, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            model_name = model_file.stem.replace('_model', '').upper()\n",
    "            models[model_name] = model\n",
    "            print(f\"  ✓ Loaded {model_name}\")\n",
    "    \n",
    "    # Load robustness results\n",
    "    robustness_path = load_path / 'robustness_results.csv'\n",
    "    if robustness_path.exists():\n",
    "        robustness_df = pd.read_csv(robustness_path)\n",
    "        robustness_results = robustness_df.to_dict('records')\n",
    "        print(f\"\\n✓ Loaded {len(robustness_results)} robustness results\")\n",
    "    \n",
    "    # Load training history\n",
    "    training_history_path = load_path / 'training_history.json'\n",
    "    if training_history_path.exists():\n",
    "        with open(training_history_path, 'r') as f:\n",
    "            training_history = json.load(f)\n",
    "        print(f\"✓ Loaded training history for {len(training_history)} models\")\n",
    "    \n",
    "    print(\"\\n✓ Load complete!\")\n",
    "else:\n",
    "    print(\"No load_path specified or path does not exist.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
